[
  {
    "authors": null,
    "categories": null,
    "content": "This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n",
    "date": 1536476400,
    "expirydate": -62135596800,
    "kind": "section",
    "lang": "en",
    "lastmod": 1536476400,
    "objectID": "c3224f3a64174f08aaf31e1f1d16ffd3",
    "permalink": "/tutorial/",
    "publishdate": "2018-09-09T00:00:00-07:00",
    "relpermalink": "/tutorial/",
    "section": "tutorial",
    "summary": "This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.",
    "tags": null,
    "title": "Overview",
    "type": "docs"
  },
  {
    "authors": null,
    "categories": [],
    "content": " In the last post I scraped data from the Walkinshaw et al. (2015) paper and re-formatted it. In this post I will clean the table with scores from the entire screen provided in the supplemental data. This includes data from lines that didn’t show a significant difference from wildtype. The benefit of having these scores is that one can get a good idea of experimental variance and it would be great if other groups would follow this lead.\nThe data is supplied as an .xlsx Excel file that we need to pull into R. This file only provides the identifier from the VDRC stock center (vdrc_id) so we will need to find the corresponding CG number which we will subsequently use to fill in the gene names. I downloaded the supplementary file into the project directory as Walkinshaw2016_suppl.xlsx and placed it in the Walkinshaw_data folder in my project directory but you can store it in any directory and include a path to that directory. We will use the read_xlsx() function from the tidyverse readxl package to pull in this data. If we inspect the file in Excel we know that the first column is blank and the next four columns contain the following data: VDRC identifier, Performance Index (PI), SEM, Date, Wing Deformity (presence or absence). The read_excel command will automatically skip the empty column and guess the type of data in the other columns.\nlibrary(dplyr) library(tidyr) library(readxl) suppl\u0026lt;-readxl::read_excel(\u0026quot;Walkinshaw_data/Walkinshaw2015_suppl.xlsx\u0026quot;) dplyr::glimpse(suppl) ## Observations: 3,207 ## Variables: 5 ## $ LINE \u0026lt;dbl\u0026gt; 105968, 105691, 105961, 105969, 105970, 105987… ## $ TRIAL \u0026lt;dbl\u0026gt; 0.42, 0.33, 0.22, 0.23, 0.52, 0.11, 0.62, 0.41… ## $ SEM \u0026lt;dbl\u0026gt; 0.0440, 0.0570, 0.0620, 0.0750, 0.0410, 0.0533… ## $ DATE \u0026lt;chr\u0026gt; \u0026quot;40718\u0026quot;, \u0026quot;40718\u0026quot;, \u0026quot;40718\u0026quot;, \u0026quot;40718\u0026quot;, \u0026quot;40718\u0026quot;, \u0026quot;… ## $ `WING DEFORMITY?` \u0026lt;chr\u0026gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… Everything looks good except for the DATE column which is coded as text instead of date. The reason that the date column contains integer values is that Excel codes dates as the number of days since 30th December 1899. This suggests that readxl may have found some non-standard dates which caused it to default to using text instead of date. To look at the full complement of dates in the dataset we can treat the DATE column as a categorical variable and list all levels of the variable.\nlevels(as.factor(suppl$DATE)) ## [1] \u0026quot;2488\u0026quot; \u0026quot;40554\u0026quot; \u0026quot;40561\u0026quot; ## [4] \u0026quot;40562\u0026quot; \u0026quot;40563\u0026quot; \u0026quot;40583\u0026quot; ## [7] \u0026quot;40666\u0026quot; \u0026quot;40668\u0026quot; \u0026quot;40674\u0026quot; ## [10] \u0026quot;40675\u0026quot; \u0026quot;40676\u0026quot; \u0026quot;40682\u0026quot; ## [13] \u0026quot;40695\u0026quot; \u0026quot;40701\u0026quot; \u0026quot;40702\u0026quot; ## [16] \u0026quot;40703\u0026quot; \u0026quot;40708\u0026quot; \u0026quot;40709\u0026quot; ## [19] \u0026quot;40710\u0026quot; \u0026quot;40711\u0026quot; \u0026quot;40715\u0026quot; ## [22] \u0026quot;40716\u0026quot; \u0026quot;40717\u0026quot; \u0026quot;40718\u0026quot; ## [25] \u0026quot;40722\u0026quot; \u0026quot;40723\u0026quot; \u0026quot;40724\u0026quot; ## [28] \u0026quot;40725\u0026quot; \u0026quot;40730\u0026quot; \u0026quot;40731\u0026quot; ## [31] \u0026quot;40732\u0026quot; \u0026quot;40736\u0026quot; \u0026quot;40737\u0026quot; ## [34] \u0026quot;40738\u0026quot; \u0026quot;40739\u0026quot; \u0026quot;40744\u0026quot; ## [37] \u0026quot;40745\u0026quot; \u0026quot;40746\u0026quot; \u0026quot;40750\u0026quot; ## [40] \u0026quot;40751\u0026quot; \u0026quot;40752\u0026quot; \u0026quot;40753\u0026quot; ## [43] \u0026quot;40758\u0026quot; \u0026quot;40759\u0026quot; \u0026quot;40760\u0026quot; ## [46] \u0026quot;40764\u0026quot; \u0026quot;40765\u0026quot; \u0026quot;40766\u0026quot; ## [49] \u0026quot;40767\u0026quot; \u0026quot;40772\u0026quot; \u0026quot;40773\u0026quot; ## [52] \u0026quot;40774\u0026quot; \u0026quot;40778\u0026quot; \u0026quot;40779\u0026quot; ## [55] \u0026quot;40780\u0026quot; \u0026quot;40781\u0026quot; \u0026quot;40786\u0026quot; ## [58] \u0026quot;40787\u0026quot; \u0026quot;40788\u0026quot; \u0026quot;40793\u0026quot; ## [61] \u0026quot;40794\u0026quot; \u0026quot;40795\u0026quot; \u0026quot;40800\u0026quot; ## [64] \u0026quot;40801\u0026quot; \u0026quot;40802\u0026quot; \u0026quot;40807\u0026quot; ## [67] \u0026quot;40808\u0026quot; \u0026quot;40809\u0026quot; \u0026quot;40814\u0026quot; ## [70] \u0026quot;40815\u0026quot; \u0026quot;40816\u0026quot; \u0026quot;40820\u0026quot; ## [73] \u0026quot;40821\u0026quot; \u0026quot;40822\u0026quot; \u0026quot;40823\u0026quot; ## [76] \u0026quot;40828\u0026quot; \u0026quot;40829\u0026quot; \u0026quot;40830\u0026quot; ## [79] \u0026quot;40834\u0026quot; \u0026quot;40835\u0026quot; \u0026quot;40836\u0026quot; ## [82] \u0026quot;40837\u0026quot; \u0026quot;40842\u0026quot; \u0026quot;40843\u0026quot; ## [85] \u0026quot;40844\u0026quot; \u0026quot;40848\u0026quot; \u0026quot;40849\u0026quot; ## [88] \u0026quot;40850\u0026quot; \u0026quot;40851\u0026quot; \u0026quot;40856\u0026quot; ## [91] \u0026quot;40857\u0026quot; \u0026quot;40858\u0026quot; \u0026quot;40862\u0026quot; ## [94] \u0026quot;40863\u0026quot; \u0026quot;40864\u0026quot; \u0026quot;40865\u0026quot; ## [97] \u0026quot;40876\u0026quot; \u0026quot;40878\u0026quot; \u0026quot;40879\u0026quot; ## [100] \u0026quot;40891\u0026quot; \u0026quot;40892\u0026quot; \u0026quot;40893\u0026quot; ## [103] \u0026quot;40898\u0026quot; \u0026quot;40899\u0026quot; \u0026quot;40905\u0026quot; ## [106] \u0026quot;40906\u0026quot; \u0026quot;40913\u0026quot; \u0026quot;40914\u0026quot; ## [109] \u0026quot;40919\u0026quot; \u0026quot;40920\u0026quot; \u0026quot;40921\u0026quot; ## [112] \u0026quot;40926\u0026quot; \u0026quot;40927\u0026quot; \u0026quot;40928\u0026quot; ## [115] \u0026quot;40932\u0026quot; \u0026quot;40933\u0026quot; \u0026quot;40934\u0026quot; ## [118] \u0026quot;40935\u0026quot; \u0026quot;40939\u0026quot; \u0026quot;40940\u0026quot; ## [121] \u0026quot;40941\u0026quot; \u0026quot;40942\u0026quot; \u0026quot;40946\u0026quot; ## [124] \u0026quot;40947\u0026quot; \u0026quot;40948\u0026quot; \u0026quot;40953\u0026quot; ## [127] \u0026quot;40954\u0026quot; \u0026quot;40955\u0026quot; \u0026quot;40956\u0026quot; ## [130] \u0026quot;40970\u0026quot; \u0026quot;40975\u0026quot; \u0026quot;40976\u0026quot; ## [133] \u0026quot;40977\u0026quot; \u0026quot;40981\u0026quot; \u0026quot;40982\u0026quot; ## [136] \u0026quot;40983\u0026quot; \u0026quot;40984\u0026quot; \u0026quot;40988\u0026quot; ## [139] \u0026quot;40989\u0026quot; \u0026quot;40990\u0026quot; \u0026quot;40991\u0026quot; ## [142] \u0026quot;40995\u0026quot; \u0026quot;40996\u0026quot; \u0026quot;40997\u0026quot; ## [145] \u0026quot;40998\u0026quot; \u0026quot;41002\u0026quot; \u0026quot;41003\u0026quot; ## [148] \u0026quot;41004\u0026quot; \u0026quot;41005\u0026quot; \u0026quot;41009\u0026quot; ## [151] \u0026quot;41010\u0026quot; \u0026quot;41011\u0026quot; \u0026quot;41012\u0026quot; ## [154] \u0026quot;41016\u0026quot; \u0026quot;41017\u0026quot; \u0026quot;41018\u0026quot; ## [157] \u0026quot;41019\u0026quot; \u0026quot;41023\u0026quot; \u0026quot;41024\u0026quot; ## [160] \u0026quot;41025\u0026quot; \u0026quot;41030\u0026quot; \u0026quot;41031\u0026quot; ## [163] \u0026quot;41032\u0026quot; \u0026quot;41033\u0026quot; \u0026quot;41037\u0026quot; ## [166] \u0026quot;41038\u0026quot; \u0026quot;41039\u0026quot; \u0026quot;41040\u0026quot; ## [169] \u0026quot;41044\u0026quot; \u0026quot;41045\u0026quot; \u0026quot;41046\u0026quot; ## [172] \u0026quot;41047\u0026quot; \u0026quot;41051\u0026quot; \u0026quot;41052\u0026quot; ## [175] \u0026quot;41053\u0026quot; \u0026quot;41054\u0026quot; \u0026quot;41055\u0026quot; ## [178] \u0026quot;41059\u0026quot; \u0026quot;41060\u0026quot; \u0026quot;41061\u0026quot; ## [181] \u0026quot;41065\u0026quot; \u0026quot;41066\u0026quot; \u0026quot;41067\u0026quot; ## [184] \u0026quot;41068\u0026quot; \u0026quot;41100\u0026quot; \u0026quot;41101\u0026quot; ## [187] \u0026quot;41102\u0026quot; \u0026quot;41103\u0026quot; \u0026quot;41107\u0026quot; ## [190] \u0026quot;41108\u0026quot; \u0026quot;41110\u0026quot; \u0026quot;41114\u0026quot; ## [193] \u0026quot;41115\u0026quot; \u0026quot;41116\u0026quot; \u0026quot;41117\u0026quot; ## [196] \u0026quot;41121\u0026quot; \u0026quot;41122\u0026quot; \u0026quot;41123\u0026quot; ## [199] \u0026quot;41124\u0026quot; \u0026quot;41128\u0026quot; \u0026quot;41129\u0026quot; ## [202] \u0026quot;41130\u0026quot; \u0026quot;41131\u0026quot; \u0026quot;41135\u0026quot; ## [205] \u0026quot;41136\u0026quot; \u0026quot;41137\u0026quot; \u0026quot;41138\u0026quot; ## [208] \u0026quot;41142\u0026quot; \u0026quot;41143\u0026quot; \u0026quot;41144\u0026quot; ## [211] \u0026quot;41145\u0026quot; \u0026quot;41149\u0026quot; \u0026quot;41150\u0026quot; ## [214] \u0026quot;41163\u0026quot; \u0026quot;41164\u0026quot; \u0026quot;41166\u0026quot; ## [217] \u0026quot;41171\u0026quot; \u0026quot;41172\u0026quot; \u0026quot;41173\u0026quot; ## [220] \u0026quot;41177\u0026quot; \u0026quot;41178\u0026quot; \u0026quot;41179\u0026quot; ## [223] \u0026quot;41180\u0026quot; \u0026quot;41184\u0026quot; \u0026quot;41185\u0026quot; ## [226] \u0026quot;41186\u0026quot; \u0026quot;41187\u0026quot; \u0026quot;41191\u0026quot; ## [229] \u0026quot;41192\u0026quot; \u0026quot;41193\u0026quot; \u0026quot;41194\u0026quot; ## [232] \u0026quot;41195\u0026quot; \u0026quot;41196\u0026quot; \u0026quot;41198\u0026quot; ## [235] \u0026quot;41199\u0026quot; \u0026quot;41200\u0026quot; \u0026quot;41201\u0026quot; ## [238] \u0026quot;41205\u0026quot; \u0026quot;41206\u0026quot; \u0026quot;41207\u0026quot; ## [241] \u0026quot;41208\u0026quot; \u0026quot;41212\u0026quot; \u0026quot;41213\u0026quot; ## [244] \u0026quot;41214\u0026quot; \u0026quot;41215\u0026quot; \u0026quot;41254\u0026quot; ## [247] \u0026quot;41255\u0026quot; \u0026quot;41256\u0026quot; \u0026quot;41291\u0026quot; ## [250] \u0026quot;41292\u0026quot; \u0026quot;41296\u0026quot; \u0026quot;41298\u0026quot; ## [253] \u0026quot;41299\u0026quot; \u0026quot;41303\u0026quot; \u0026quot;41305\u0026quot; ## [256] \u0026quot;41306\u0026quot; \u0026quot;41310\u0026quot; \u0026quot;41311\u0026quot; ## [259] \u0026quot;41312\u0026quot; \u0026quot;41313\u0026quot; \u0026quot;41317\u0026quot; ## [262] \u0026quot;41318\u0026quot; \u0026quot;41319\u0026quot; \u0026quot;41320\u0026quot; ## [265] \u0026quot;41353\u0026quot; \u0026quot;41354\u0026quot; \u0026quot;41355\u0026quot; ## [268] \u0026quot;41401\u0026quot; \u0026quot;41402\u0026quot; \u0026quot;6/8/11\u0026quot; ## [271] \u0026quot;9\u0026quot; \u0026quot;Date\u0026quot; \u0026quot;Date: 5/25/2011\u0026quot; ## [274] \u0026quot;Date: 5/26/2011\u0026quot; \u0026quot;Date: 5/27/2011\u0026quot; \u0026quot;Date: 6/01/2011\u0026quot; ## [277] \u0026quot;Date: 6/01/2012\u0026quot; \u0026quot;Date: 6/01/2013\u0026quot; \u0026quot;Date: 6/01/2014\u0026quot; ## [280] \u0026quot;Date:09/08/2011\u0026quot; We see from this list that in addition to the numerical dates encoded for Excel several other formats are also represented. We want to pay particular attention to two types of formats: 1. where the cell is empty and 2. where the cell contains actual dates but which are written in text format. We will replace empty cells with the string “NA” so that regular expressions will work when we use the stringr package in subsequent steps. We will re-format dates that are coded as text into a standard format and then use the as.date() function to convert them into a date object. Finally we will convert the Excel-formatted dates and merge all data to build a final table.\nIn the following snippet we change the missing data to “NA” and then fix the dates coded as text.\nlibrary(stringr) # replace missing dates with NA suppl \u0026lt;- suppl %\u0026gt;% mutate(DATE=replace_na(DATE,\u0026quot;NA\u0026quot;)) # copy text formatted dates to a separate table suppl_date \u0026lt;- suppl %\u0026gt;% filter(str_detect(DATE,\u0026quot;[0-9]/\u0026quot;)) # change 4 digit years to 2 digit years using a regular expression with gsub suppl_date$DATE \u0026lt;- gsub(\u0026quot;(20)([0-9][0-9])\u0026quot;, \u0026quot;\\\\2\u0026quot;, as.character(suppl_date$DATE)) # remove the text \u0026quot;Date:\u0026quot; and convert to a date object suppl_date \u0026lt;- suppl_date %\u0026gt;% mutate(DATE=str_replace(DATE, \u0026quot;Date:\u0026quot;,\u0026quot;\u0026quot;)) %\u0026gt;% mutate(DATE=as.Date(DATE, format = \u0026quot;%m/%d/%y\u0026quot;)) levels(as.factor(suppl_date$DATE)) ## [1] \u0026quot;2011-05-25\u0026quot; \u0026quot;2011-05-26\u0026quot; \u0026quot;2011-05-27\u0026quot; \u0026quot;2011-06-01\u0026quot; \u0026quot;2011-06-08\u0026quot; ## [6] \u0026quot;2011-09-08\u0026quot; \u0026quot;2012-06-01\u0026quot; \u0026quot;2013-06-01\u0026quot; \u0026quot;2014-06-01\u0026quot; In the next snippet we filter out the rows that we have already converted to dates and change the Excel encoded ones to R date objects. The cells that don’t contain dates will be coerced into NAs implying missing data.\nsuppl \u0026lt;- suppl %\u0026gt;% # filter out dates encoded as text filter(!str_detect(DATE, \u0026quot;[0-9]/\u0026quot;)) %\u0026gt;% # convert Excel dates into R dates mutate(DATE=as.Date(as.integer(DATE), origin = \u0026quot;1899-12-30\u0026quot;)) Now we can re-combine the two data tables into a single table with consistent date formatting.\nsuppl \u0026lt;- bind_rows(suppl, suppl_date) levels(as.factor(suppl$DATE)) ## [1] \u0026quot;1900-01-08\u0026quot; \u0026quot;1906-10-23\u0026quot; \u0026quot;2011-01-11\u0026quot; \u0026quot;2011-01-18\u0026quot; \u0026quot;2011-01-19\u0026quot; ## [6] \u0026quot;2011-01-20\u0026quot; \u0026quot;2011-02-09\u0026quot; \u0026quot;2011-05-03\u0026quot; \u0026quot;2011-05-05\u0026quot; \u0026quot;2011-05-11\u0026quot; ## [11] \u0026quot;2011-05-12\u0026quot; \u0026quot;2011-05-13\u0026quot; \u0026quot;2011-05-19\u0026quot; \u0026quot;2011-05-25\u0026quot; \u0026quot;2011-05-26\u0026quot; ## [16] \u0026quot;2011-05-27\u0026quot; \u0026quot;2011-06-01\u0026quot; \u0026quot;2011-06-07\u0026quot; \u0026quot;2011-06-08\u0026quot; \u0026quot;2011-06-09\u0026quot; ## [21] \u0026quot;2011-06-14\u0026quot; \u0026quot;2011-06-15\u0026quot; \u0026quot;2011-06-16\u0026quot; \u0026quot;2011-06-17\u0026quot; \u0026quot;2011-06-21\u0026quot; ## [26] \u0026quot;2011-06-22\u0026quot; \u0026quot;2011-06-23\u0026quot; \u0026quot;2011-06-24\u0026quot; \u0026quot;2011-06-28\u0026quot; \u0026quot;2011-06-29\u0026quot; ## [31] \u0026quot;2011-06-30\u0026quot; \u0026quot;2011-07-01\u0026quot; \u0026quot;2011-07-06\u0026quot; \u0026quot;2011-07-07\u0026quot; \u0026quot;2011-07-08\u0026quot; ## [36] \u0026quot;2011-07-12\u0026quot; \u0026quot;2011-07-13\u0026quot; \u0026quot;2011-07-14\u0026quot; \u0026quot;2011-07-15\u0026quot; \u0026quot;2011-07-20\u0026quot; ## [41] \u0026quot;2011-07-21\u0026quot; \u0026quot;2011-07-22\u0026quot; \u0026quot;2011-07-26\u0026quot; \u0026quot;2011-07-27\u0026quot; \u0026quot;2011-07-28\u0026quot; ## [46] \u0026quot;2011-07-29\u0026quot; \u0026quot;2011-08-03\u0026quot; \u0026quot;2011-08-04\u0026quot; \u0026quot;2011-08-05\u0026quot; \u0026quot;2011-08-09\u0026quot; ## [51] \u0026quot;2011-08-10\u0026quot; \u0026quot;2011-08-11\u0026quot; \u0026quot;2011-08-12\u0026quot; \u0026quot;2011-08-17\u0026quot; \u0026quot;2011-08-18\u0026quot; ## [56] \u0026quot;2011-08-19\u0026quot; \u0026quot;2011-08-23\u0026quot; \u0026quot;2011-08-24\u0026quot; \u0026quot;2011-08-25\u0026quot; \u0026quot;2011-08-26\u0026quot; ## [61] \u0026quot;2011-08-31\u0026quot; \u0026quot;2011-09-01\u0026quot; \u0026quot;2011-09-02\u0026quot; \u0026quot;2011-09-07\u0026quot; \u0026quot;2011-09-08\u0026quot; ## [66] \u0026quot;2011-09-09\u0026quot; \u0026quot;2011-09-14\u0026quot; \u0026quot;2011-09-15\u0026quot; \u0026quot;2011-09-16\u0026quot; \u0026quot;2011-09-21\u0026quot; ## [71] \u0026quot;2011-09-22\u0026quot; \u0026quot;2011-09-23\u0026quot; \u0026quot;2011-09-28\u0026quot; \u0026quot;2011-09-29\u0026quot; \u0026quot;2011-09-30\u0026quot; ## [76] \u0026quot;2011-10-04\u0026quot; \u0026quot;2011-10-05\u0026quot; \u0026quot;2011-10-06\u0026quot; \u0026quot;2011-10-07\u0026quot; \u0026quot;2011-10-12\u0026quot; ## [81] \u0026quot;2011-10-13\u0026quot; \u0026quot;2011-10-14\u0026quot; \u0026quot;2011-10-18\u0026quot; \u0026quot;2011-10-19\u0026quot; \u0026quot;2011-10-20\u0026quot; ## [86] \u0026quot;2011-10-21\u0026quot; \u0026quot;2011-10-26\u0026quot; \u0026quot;2011-10-27\u0026quot; \u0026quot;2011-10-28\u0026quot; \u0026quot;2011-11-01\u0026quot; ## [91] \u0026quot;2011-11-02\u0026quot; \u0026quot;2011-11-03\u0026quot; \u0026quot;2011-11-04\u0026quot; \u0026quot;2011-11-09\u0026quot; \u0026quot;2011-11-10\u0026quot; ## [96] \u0026quot;2011-11-11\u0026quot; \u0026quot;2011-11-15\u0026quot; \u0026quot;2011-11-16\u0026quot; \u0026quot;2011-11-17\u0026quot; \u0026quot;2011-11-18\u0026quot; ## [101] \u0026quot;2011-11-29\u0026quot; \u0026quot;2011-12-01\u0026quot; \u0026quot;2011-12-02\u0026quot; \u0026quot;2011-12-14\u0026quot; \u0026quot;2011-12-15\u0026quot; ## [106] \u0026quot;2011-12-16\u0026quot; \u0026quot;2011-12-21\u0026quot; \u0026quot;2011-12-22\u0026quot; \u0026quot;2011-12-28\u0026quot; \u0026quot;2011-12-29\u0026quot; ## [111] \u0026quot;2012-01-05\u0026quot; \u0026quot;2012-01-06\u0026quot; \u0026quot;2012-01-11\u0026quot; \u0026quot;2012-01-12\u0026quot; \u0026quot;2012-01-13\u0026quot; ## [116] \u0026quot;2012-01-18\u0026quot; \u0026quot;2012-01-19\u0026quot; \u0026quot;2012-01-20\u0026quot; \u0026quot;2012-01-24\u0026quot; \u0026quot;2012-01-25\u0026quot; ## [121] \u0026quot;2012-01-26\u0026quot; \u0026quot;2012-01-27\u0026quot; \u0026quot;2012-01-31\u0026quot; \u0026quot;2012-02-01\u0026quot; \u0026quot;2012-02-02\u0026quot; ## [126] \u0026quot;2012-02-03\u0026quot; \u0026quot;2012-02-07\u0026quot; \u0026quot;2012-02-08\u0026quot; \u0026quot;2012-02-09\u0026quot; \u0026quot;2012-02-14\u0026quot; ## [131] \u0026quot;2012-02-15\u0026quot; \u0026quot;2012-02-16\u0026quot; \u0026quot;2012-02-17\u0026quot; \u0026quot;2012-03-02\u0026quot; \u0026quot;2012-03-07\u0026quot; ## [136] \u0026quot;2012-03-08\u0026quot; \u0026quot;2012-03-09\u0026quot; \u0026quot;2012-03-13\u0026quot; \u0026quot;2012-03-14\u0026quot; \u0026quot;2012-03-15\u0026quot; ## [141] \u0026quot;2012-03-16\u0026quot; \u0026quot;2012-03-20\u0026quot; \u0026quot;2012-03-21\u0026quot; \u0026quot;2012-03-22\u0026quot; \u0026quot;2012-03-23\u0026quot; ## [146] \u0026quot;2012-03-27\u0026quot; \u0026quot;2012-03-28\u0026quot; \u0026quot;2012-03-29\u0026quot; \u0026quot;2012-03-30\u0026quot; \u0026quot;2012-04-03\u0026quot; ## [151] \u0026quot;2012-04-04\u0026quot; \u0026quot;2012-04-05\u0026quot; \u0026quot;2012-04-06\u0026quot; \u0026quot;2012-04-10\u0026quot; \u0026quot;2012-04-11\u0026quot; ## [156] \u0026quot;2012-04-12\u0026quot; \u0026quot;2012-04-13\u0026quot; \u0026quot;2012-04-17\u0026quot; \u0026quot;2012-04-18\u0026quot; \u0026quot;2012-04-19\u0026quot; ## [161] \u0026quot;2012-04-20\u0026quot; \u0026quot;2012-04-24\u0026quot; \u0026quot;2012-04-25\u0026quot; \u0026quot;2012-04-26\u0026quot; \u0026quot;2012-05-01\u0026quot; ## [166] \u0026quot;2012-05-02\u0026quot; \u0026quot;2012-05-03\u0026quot; \u0026quot;2012-05-04\u0026quot; \u0026quot;2012-05-08\u0026quot; \u0026quot;2012-05-09\u0026quot; ## [171] \u0026quot;2012-05-10\u0026quot; \u0026quot;2012-05-11\u0026quot; \u0026quot;2012-05-15\u0026quot; \u0026quot;2012-05-16\u0026quot; \u0026quot;2012-05-17\u0026quot; ## [176] \u0026quot;2012-05-18\u0026quot; \u0026quot;2012-05-22\u0026quot; \u0026quot;2012-05-23\u0026quot; \u0026quot;2012-05-24\u0026quot; \u0026quot;2012-05-25\u0026quot; ## [181] \u0026quot;2012-05-26\u0026quot; \u0026quot;2012-05-30\u0026quot; \u0026quot;2012-05-31\u0026quot; \u0026quot;2012-06-01\u0026quot; \u0026quot;2012-06-05\u0026quot; ## [186] \u0026quot;2012-06-06\u0026quot; \u0026quot;2012-06-07\u0026quot; \u0026quot;2012-06-08\u0026quot; \u0026quot;2012-07-10\u0026quot; \u0026quot;2012-07-11\u0026quot; ## [191] \u0026quot;2012-07-12\u0026quot; \u0026quot;2012-07-13\u0026quot; \u0026quot;2012-07-17\u0026quot; \u0026quot;2012-07-18\u0026quot; \u0026quot;2012-07-20\u0026quot; ## [196] \u0026quot;2012-07-24\u0026quot; \u0026quot;2012-07-25\u0026quot; \u0026quot;2012-07-26\u0026quot; \u0026quot;2012-07-27\u0026quot; \u0026quot;2012-07-31\u0026quot; ## [201] \u0026quot;2012-08-01\u0026quot; \u0026quot;2012-08-02\u0026quot; \u0026quot;2012-08-03\u0026quot; \u0026quot;2012-08-07\u0026quot; \u0026quot;2012-08-08\u0026quot; ## [206] \u0026quot;2012-08-09\u0026quot; \u0026quot;2012-08-10\u0026quot; \u0026quot;2012-08-14\u0026quot; \u0026quot;2012-08-15\u0026quot; \u0026quot;2012-08-16\u0026quot; ## [211] \u0026quot;2012-08-17\u0026quot; \u0026quot;2012-08-21\u0026quot; \u0026quot;2012-08-22\u0026quot; \u0026quot;2012-08-23\u0026quot; \u0026quot;2012-08-24\u0026quot; ## [216] \u0026quot;2012-08-28\u0026quot; \u0026quot;2012-08-29\u0026quot; \u0026quot;2012-09-11\u0026quot; \u0026quot;2012-09-12\u0026quot; \u0026quot;2012-09-14\u0026quot; ## [221] \u0026quot;2012-09-19\u0026quot; \u0026quot;2012-09-20\u0026quot; \u0026quot;2012-09-21\u0026quot; \u0026quot;2012-09-25\u0026quot; \u0026quot;2012-09-26\u0026quot; ## [226] \u0026quot;2012-09-27\u0026quot; \u0026quot;2012-09-28\u0026quot; \u0026quot;2012-10-02\u0026quot; \u0026quot;2012-10-03\u0026quot; \u0026quot;2012-10-04\u0026quot; ## [231] \u0026quot;2012-10-05\u0026quot; \u0026quot;2012-10-09\u0026quot; \u0026quot;2012-10-10\u0026quot; \u0026quot;2012-10-11\u0026quot; \u0026quot;2012-10-12\u0026quot; ## [236] \u0026quot;2012-10-13\u0026quot; \u0026quot;2012-10-14\u0026quot; \u0026quot;2012-10-16\u0026quot; \u0026quot;2012-10-17\u0026quot; \u0026quot;2012-10-18\u0026quot; ## [241] \u0026quot;2012-10-19\u0026quot; \u0026quot;2012-10-23\u0026quot; \u0026quot;2012-10-24\u0026quot; \u0026quot;2012-10-25\u0026quot; \u0026quot;2012-10-26\u0026quot; ## [246] \u0026quot;2012-10-30\u0026quot; \u0026quot;2012-10-31\u0026quot; \u0026quot;2012-11-01\u0026quot; \u0026quot;2012-11-02\u0026quot; \u0026quot;2012-12-11\u0026quot; ## [251] \u0026quot;2012-12-12\u0026quot; \u0026quot;2012-12-13\u0026quot; \u0026quot;2013-01-17\u0026quot; \u0026quot;2013-01-18\u0026quot; \u0026quot;2013-01-22\u0026quot; ## [256] \u0026quot;2013-01-24\u0026quot; \u0026quot;2013-01-25\u0026quot; \u0026quot;2013-01-29\u0026quot; \u0026quot;2013-01-31\u0026quot; \u0026quot;2013-02-01\u0026quot; ## [261] \u0026quot;2013-02-05\u0026quot; \u0026quot;2013-02-06\u0026quot; \u0026quot;2013-02-07\u0026quot; \u0026quot;2013-02-08\u0026quot; \u0026quot;2013-02-12\u0026quot; ## [266] \u0026quot;2013-02-13\u0026quot; \u0026quot;2013-02-14\u0026quot; \u0026quot;2013-02-15\u0026quot; \u0026quot;2013-03-20\u0026quot; \u0026quot;2013-03-21\u0026quot; ## [271] \u0026quot;2013-03-22\u0026quot; \u0026quot;2013-05-07\u0026quot; \u0026quot;2013-05-08\u0026quot; \u0026quot;2013-06-01\u0026quot; \u0026quot;2014-06-01\u0026quot; There are two sets of data points from the years 1900 and 1906 which clearly cannot be correct! We will leave them as is for now but these should be filtered out before conducting any time-dependent analysis.\nNext we want to format the supplementary data to match the format of the significant_lines table that we constructed in the previous post. To do this we will change the column headers and also code the presence or absence of physical abnormality with “+” and “-” respectively. Originally I had thought about rounding the PI and SEM to two decimal places to match the data from the text of the article. However, the PI and SEM scores in the supplementary table are inconsistently rounded which may help us with subsetting the data later on so we will leave them in their current state for now.\nsuppl\u0026lt;-suppl %\u0026gt;% rename(vdrc_id=1, primary_PI=2, primary_SEM=3, date=4, physical_abnormality=5) %\u0026gt;% # mutate(primary_PI = round(primary_PI, 2)) %\u0026gt;% # mutate(primary_SEM = abs(round(primary_SEM, 2))) %\u0026gt;% mutate(physical_abnormality = replace(physical_abnormality, physical_abnormality == \u0026quot;Present\u0026quot;, \u0026quot;+\u0026quot;)) %\u0026gt;% mutate(physical_abnormality = replace(physical_abnormality, is.na(physical_abnormality), \u0026quot;-\u0026quot;)) %\u0026gt;% arrange(vdrc_id) head(suppl, 10) ## # A tibble: 10 x 5 ## vdrc_id primary_PI primary_SEM date physical_abnormality ## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;date\u0026gt; \u0026lt;chr\u0026gt; ## 1 3047 0.34 0.09 2011-07-29 - ## 2 3424 0.299 0.147 2013-01-25 - ## 3 5070 0.401 0.141 2012-06-08 - ## 4 6212 0.274 0.102 2011-12-14 - ## 5 7219 0.41 0.044 2012-10-23 - ## 6 7220 0.120 0.127 2013-03-20 - ## 7 9524 0.3 0.0614 2011-06-28 - ## 8 10836 0.492 0.0697 2013-02-12 - ## 9 11471 0.16 0.09 2011-07-28 - ## 10 11784 0.43 0.11 2011-07-29 - The only identifiers in this table are VDRC Identifiers (vdrc_id) which means we have to use additional resources to find the name of the targeted gene. We can add identifiers such as gene symbols, Entrez Gene IDs and Flybase IDs that will make it easier to use this data. The VDRC (Vienna Drosophila Resource Center) provides a complete list of their lines in a table that contains several useful identifiers: https://stockcenter.vdrc.at/control/fullCatalogueExcel. We will use the read_xls() command from the readxl package to load this file into a new dataframe. The read_xls() command guesses the data types but for this file it had trouble recognizing the data types in two of the columns. It turns out that these columns contained no data for the lines used in this screen so we can skip those columns using the following command.\nVDRC \u0026lt;- read_xls(\u0026quot;Walkinshaw_data/REPORT_VdrcCatalogue.xls\u0026quot;, col_types = c(\u0026quot;guess\u0026quot;, \u0026quot;guess\u0026quot;, \u0026quot;guess\u0026quot;, \u0026quot;guess\u0026quot;, \u0026quot;guess\u0026quot;,\u0026quot;guess\u0026quot;,\u0026quot;guess\u0026quot;,\u0026quot;guess\u0026quot;,\u0026quot;guess\u0026quot;, \u0026quot;guess\u0026quot;,\u0026quot;guess\u0026quot;,\u0026quot;skip\u0026quot;,\u0026quot;skip\u0026quot;,\u0026quot;guess\u0026quot;,\u0026quot;guess\u0026quot;)) glimpse(VDRC) ## Observations: 33,286 ## Variables: 13 ## $ `vdrc id` \u0026lt;dbl\u0026gt; 836, 837, 839, 841, 842, 843, 845, 846, 848, 849… ## $ `construct id` \u0026lt;dbl\u0026gt; 37, 37, 38, 44, 45, 45, 47, 47, 49, 49, 51, 52, … ## $ library \u0026lt;chr\u0026gt; \u0026quot;GD\u0026quot;, \u0026quot;GD\u0026quot;, \u0026quot;GD\u0026quot;, \u0026quot;GD\u0026quot;, \u0026quot;GD\u0026quot;, \u0026quot;GD\u0026quot;, \u0026quot;GD\u0026quot;, \u0026quot;GD\u0026quot;, … ## $ `cg number` \u0026lt;chr\u0026gt; \u0026quot;CG5819\u0026quot;, \u0026quot;CG5819\u0026quot;, \u0026quot;CG7121\u0026quot;, \u0026quot;CG4007\u0026quot;, \u0026quot;CG14396… ## $ `on targets` \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ `off targets` \u0026lt;dbl\u0026gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, … ## $ s19 \u0026lt;dbl\u0026gt; 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, … ## $ `can repeats` \u0026lt;dbl\u0026gt; 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, … ## $ viability \u0026lt;chr\u0026gt; \u0026quot;viable\u0026quot;, \u0026quot;viable\u0026quot;, \u0026quot;viable\u0026quot;, \u0026quot;sterile\u0026quot;, \u0026quot;steril… ## $ `chromosome nr` \u0026lt;dbl\u0026gt; 3, 1, 2, 3, 2, 3, 3, 2, 2, 2, 3, 3, 3, 1, 3, 2, … ## $ status \u0026lt;chr\u0026gt; \u0026quot;available\u0026quot;, \u0026quot;unavailable\u0026quot;, \u0026quot;available\u0026quot;, \u0026quot;unavai… ## $ `fb numbers` \u0026lt;chr\u0026gt; \u0026quot;\\nFBgn0034717\u0026quot;, \u0026quot;FBgn0034717\u0026quot;, \u0026quot;FBgn0026760\u0026quot;, \u0026quot;… ## $ synonyms \u0026lt;chr\u0026gt; \u0026quot;\\ntartan/capricious-like\\nCT18212\\n5819\u0026quot;, \u0026quot;5819… There are 33286 lines in the dataset and a number of identifiers are available for each of them. However, many of them, such as the FlyBase Gene (FBgn) and gene name identifiers, can be unstable and this table lists a number of synonyms including ones that have been discarded. On the other hand only a single CG number is listed per line which we can later use in conjunction with the Ensembl BioMart webservice from the European Bioinformatics Institute (EBI) to add the current versions of the other identifiers. So for the time being we will discard the FBgn numbers and synonyms encoded by the last two columns of the dataframe and rename other columns so that they match the tables from the article. We will then use an inner join to combine the two datasets but discard all rows of the VDRC data that don’t have a counterpart in the supplementary data from the paper.\nall_lines\u0026lt;-VDRC %\u0026gt;% dplyr::select(1:11) %\u0026gt;% dplyr::rename(vdrc_id=1,construct_id=2,lib=3,cg_number=4,on_targets=5, off_targets=6,s19=7,can_repeats=8,viability=9,chromosome_nr=10, status=11) %\u0026gt;% dplyr::inner_join(suppl, by=\u0026quot;vdrc_id\u0026quot;) glimpse(all_lines) ## Observations: 3,206 ## Variables: 15 ## $ vdrc_id \u0026lt;dbl\u0026gt; 3047, 3424, 5070, 6212, 7219, 7220, 10836, … ## $ construct_id \u0026lt;dbl\u0026gt; 2537, 394, 2336, 1469, 715, 715, 4401, 732,… ## $ lib \u0026lt;chr\u0026gt; \u0026quot;GD\u0026quot;, \u0026quot;GD\u0026quot;, \u0026quot;GD\u0026quot;, \u0026quot;GD\u0026quot;, \u0026quot;GD\u0026quot;, \u0026quot;GD\u0026quot;, \u0026quot;GD\u0026quot;, \u0026quot;… ## $ cg_number \u0026lt;chr\u0026gt; \u0026quot;CG17348\u0026quot;, \u0026quot;CG8451\u0026quot;, \u0026quot;CG8339\u0026quot;, \u0026quot;CG33956\u0026quot;, \u0026quot;… ## $ on_targets \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ off_targets \u0026lt;dbl\u0026gt; 0, 0, 0, 1, 1, 1, 2, 1, 0, 0, 0, 1, 0, 0, 0… ## $ s19 \u0026lt;dbl\u0026gt; 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 0.99, 1… ## $ can_repeats \u0026lt;dbl\u0026gt; 2, 2, 2, 3, 2, 2, 5, 3, 3, 2, 2, 3, 4, 2, 2… ## $ viability \u0026lt;chr\u0026gt; \u0026quot;viable\u0026quot;, \u0026quot;viable\u0026quot;, \u0026quot;viable\u0026quot;, \u0026quot;lethal\u0026quot;, \u0026quot;vi… ## $ chromosome_nr \u0026lt;dbl\u0026gt; 2, 3, 3, 3, 3, 3, 2, 2, 1, 3, 2, 2, 2, 3, 3… ## $ status \u0026lt;chr\u0026gt; \u0026quot;available\u0026quot;, \u0026quot;available\u0026quot;, \u0026quot;available\u0026quot;, \u0026quot;ava… ## $ primary_PI \u0026lt;dbl\u0026gt; 0.34000000, 0.29904285, 0.40079441, 0.27353… ## $ primary_SEM \u0026lt;dbl\u0026gt; 0.09000000, 0.14749334, 0.14076967, 0.10236… ## $ date \u0026lt;date\u0026gt; 2011-07-29, 2013-01-25, 2012-06-08, 2011-1… ## $ physical_abnormality \u0026lt;chr\u0026gt; \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;… Next we will use the biomart service of EBI to get current Entrez Gene IDs, FlyBase numbers and gene symbols using the list of CG numbers from the all_lines dataframe. The biomaRt package allows very convenient programmatic access to BioMart.\nlibrary(biomaRt) # connect to the ensembl Drosophila melanogaster database v.90 ensembl = useEnsembl(biomart=\u0026quot;ensembl\u0026quot;, dataset=\u0026quot;dmelanogaster_gene_ensembl\u0026quot;, version=90) # get gene names, entrez gene id, FlyBase gene id, and FlyBase CG number # use the cg_number column from all_lines for identifiers genenames \u0026lt;- getBM(attributes=c(\u0026#39;external_gene_name\u0026#39;,\u0026#39;entrezgene\u0026#39;,\u0026#39;flybase_gene_id\u0026#39;, \u0026#39;flybasecgid_gene\u0026#39;), filters = \u0026#39;flybasecgid_gene\u0026#39;, values = unique(all_lines$cg_number), mart = ensembl) Let’s check the size of the genenames dataframe to get an idea if we were able to map every identifier. First we will check how many unique CG numbers there are in the Walkinshaw dataset:\nall_lines$cg_number %\u0026gt;% unique() %\u0026gt;% length() ## [1] 2999 Now we want to check how many ids we got from genenames:\ndim(genenames) ## [1] 3020 4 It is odd that we have 3020 identifiers from genenames compared to 2999 that we expected so we definitely want to see which CG numbers are being mapped to multiple identifiers. First we will look for CG numbers that are duplicated and store them in a vector called dup. Next we will filter the genenames dataframe to find rows which have duplicated CG ids:\ndup\u0026lt;- genenames[which(duplicated(genenames$flybasecgid_gene)==TRUE),]$flybasecgid_gene dplyr::filter(genenames, flybasecgid_gene %in% dup) ## external_gene_name entrezgene flybase_gene_id flybasecgid_gene ## 1 Lcp65Ab2 48382 FBgn0020643 CG18773 ## 2 Lcp65Ab2 48381 FBgn0020643 CG18773 ## 3 His3:CG31613 3772163 FBgn0051613 CG31613 ## 4 His3:CG31613 3772198 FBgn0051613 CG31613 ## 5 His3:CG31613 3772374 FBgn0051613 CG31613 ## 6 His3:CG31613 3772173 FBgn0051613 CG31613 ## 7 His3:CG31613 3772032 FBgn0051613 CG31613 ## 8 His3:CG31613 3771771 FBgn0051613 CG31613 ## 9 His3:CG31613 3771959 FBgn0051613 CG31613 ## 10 His3:CG31613 3772421 FBgn0051613 CG31613 ## 11 His3:CG31613 3772607 FBgn0051613 CG31613 ## 12 His3:CG31613 3772517 FBgn0051613 CG31613 ## 13 His3:CG31613 3772231 FBgn0051613 CG31613 ## 14 His3:CG31613 318847 FBgn0051613 CG31613 ## 15 His3:CG31613 3772149 FBgn0051613 CG31613 ## 16 His3:CG31613 3772189 FBgn0051613 CG31613 ## 17 His3:CG31613 3772552 FBgn0051613 CG31613 ## 18 His3:CG31613 3771723 FBgn0051613 CG31613 ## 19 His3:CG31613 3772191 FBgn0051613 CG31613 ## 20 His3:CG31613 3772370 FBgn0051613 CG31613 ## 21 His3:CG31613 3772619 FBgn0051613 CG31613 ## 22 His3:CG31613 3771792 FBgn0051613 CG31613 ## 23 His3:CG31613 3772518 FBgn0051613 CG31613 ## 24 His3:CG31613 3771729 FBgn0051613 CG31613 ## 25 His3:CG31613 3772489 FBgn0051613 CG31613 There are two CG numbers that have multiple Entrez IDs. Let’s deal with CG31613 first. In Eukaryotes, that is organisms that have a nucleus, DNA is wrapped around structures called nucleosomes which are octamers that consist of 4 different histone proteins of which Histone3 is one. Since this is a highly abundant and necessary protein, there are a total of 23 genes that encode for it. Five of these genes, including CG31613 are present in a cluster on chromosome 2L. In the Walkinshaw screen, 5 lines were used to target this gene cluster but since they all have an identical DNA sequence, they used a single CG number to refer to these lines. The Entrez Gene IDs probably reflect this ambiguity. CG18773, the second ambiguous CG number encodes a cuticular protein for which there are two identical genes in the fly genome. We will deal with both of these CG numbers by removing the Entrez IDs since they add no significant information to the dataset.\n# filter rows in genenames with CG ids not in the dup vector (note the ! operator) genenames \u0026lt;- dplyr::filter(genenames, !(flybasecgid_gene %in% dup)) dim(genenames) ## [1] 2995 4 The genenames dataframe now has 2995 rows while the all_lines dataframe has 2999. Taking into account that we removed CG18733 and CG31613, we should have 2997 rows in genenames. We can check if there are any CG numbers that were not mapped by the BioMart webservice:\nall_lines$cg_number[(all_lines$cg_number %in% genenames$flybasecgid_gene)==FALSE] ## [1] \u0026quot;CGnone\u0026quot; \u0026quot;CGnone\u0026quot; \u0026quot;CGnone\u0026quot; \u0026quot;CGnone\u0026quot; \u0026quot;CGnone\u0026quot; \u0026quot;CGnone\u0026quot; \u0026quot;CGnone\u0026quot; ## [8] \u0026quot;CGnone\u0026quot; \u0026quot;CGnone\u0026quot; \u0026quot;CGnone\u0026quot; \u0026quot;CGnone\u0026quot; \u0026quot;CGnone\u0026quot; \u0026quot;CGnone\u0026quot; \u0026quot;CGnone\u0026quot; ## [15] \u0026quot;CG40378\u0026quot; \u0026quot;CGnone\u0026quot; \u0026quot;CG18773\u0026quot; \u0026quot;CG31613\u0026quot; \u0026quot;CG31613\u0026quot; \u0026quot;CG31613\u0026quot; \u0026quot;CG31613\u0026quot; ## [22] \u0026quot;CG31613\u0026quot; \u0026quot;CGnone\u0026quot; \u0026quot;CGnone\u0026quot; \u0026quot;CGnone\u0026quot; \u0026quot;CGnone\u0026quot; \u0026quot;CGnone\u0026quot; \u0026quot;CGnone\u0026quot; ## [29] \u0026quot;CGnone\u0026quot; \u0026quot;CGnone\u0026quot; \u0026quot;CGnone\u0026quot; There are indeed 2 CG numbers that were not mapped. One is called CGnone of which there are 24 instances and the other one is CG40378. The VDRC table shows that most of these lines are no longer available with one exception, 104930. I found the genomic DNA sequence for this construct on the VDRC website and used BLAST on the FlyBase website to find where it mapped on the Drosophila genome. It turns out this line targets a long non-coding RNA called lncRNA:CR13130. The other unmapped CG number, CG40378, has been changed to CG45781. We will change CG40378 to CG45781, for stock 104930 we will change CGnone to CR13130, remove the other rows with CGnone and use BioMart to fill in the rest of the information.\nall_lines \u0026lt;- mutate(all_lines, cg_number = ifelse(cg_number == \u0026quot;CG40378\u0026quot;, \u0026quot;CG45781\u0026quot;, cg_number)) all_lines \u0026lt;- mutate(all_lines, cg_number = ifelse(vdrc_id == 104930, \u0026quot;CR13130\u0026quot;, cg_number)) all_lines \u0026lt;- all_lines %\u0026gt;% filter(cg_number != \u0026quot;CGnone\u0026quot;) genenames \u0026lt;- getBM(attributes=c(\u0026#39;external_gene_name\u0026#39;,\u0026#39;flybase_gene_id\u0026#39;, \u0026#39;flybasecgid_gene\u0026#39;), filters = \u0026#39;flybasecgid_gene\u0026#39;, values = unique(all_lines$cg_number), mart = ensembl) dim(genenames) ## [1] 2999 3 Now we have the same number of unique rows in the genenames dataframe as in the all_lines dataframe. We can also confirm that we have removed all of the CGnone rows. We can add the gene symbols and flybase gene ids to the all_lines dataframe using an inner join with the genenames dataframe.\nall_lines \u0026lt;- inner_join(genenames, all_lines, by=c(\u0026quot;flybasecgid_gene\u0026quot;=\u0026quot;cg_number\u0026quot;)) %\u0026gt;% dplyr::select(4,1,2,3,5:19) %\u0026gt;% rename(cg_number=flybasecgid_gene) glimpse(all_lines) ## Observations: 3,183 ## Variables: 19 ## $ vdrc_id \u0026lt;dbl\u0026gt; 101031, 28150, 100587, 100007, 100226, 1002… ## $ external_gene_name \u0026lt;chr\u0026gt; \u0026quot;Zip71B\u0026quot;, \u0026quot;Galphai\u0026quot;, \u0026quot;emc\u0026quot;, \u0026quot;CG10137\u0026quot;, \u0026quot;x16… ## $ flybase_gene_id \u0026lt;chr\u0026gt; \u0026quot;FBgn0036461\u0026quot;, \u0026quot;FBgn0001104\u0026quot;, \u0026quot;FBgn0000575\u0026quot;… ## $ cg_number \u0026lt;chr\u0026gt; \u0026quot;CG10006\u0026quot;, \u0026quot;CG10060\u0026quot;, \u0026quot;CG1007\u0026quot;, \u0026quot;CG10137\u0026quot;, … ## $ construct_id \u0026lt;dbl\u0026gt; 106638, 12576, 108316, 102904, 107171, 1057… ## $ library \u0026lt;chr\u0026gt; \u0026quot;KK\u0026quot;, \u0026quot;GD\u0026quot;, \u0026quot;KK\u0026quot;, \u0026quot;KK\u0026quot;, \u0026quot;KK\u0026quot;, \u0026quot;KK\u0026quot;, \u0026quot;KK\u0026quot;, \u0026quot;… ## $ on_targets \u0026lt;dbl\u0026gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ off_targets \u0026lt;dbl\u0026gt; 2, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0… ## $ s19 \u0026lt;dbl\u0026gt; 0.99, 1.00, 1.00, 0.99, 1.00, 1.00, 1.00, 1… ## $ can_repeats \u0026lt;dbl\u0026gt; 3, 2, 2, 2, 3, 3, 3, 3, 2, 3, 2, 3, 2, 2, 3… ## $ viability \u0026lt;chr\u0026gt; \u0026quot;viable\u0026quot;, \u0026quot;lethal\u0026quot;, \u0026quot;viable\u0026quot;, \u0026quot;viable\u0026quot;, \u0026quot;vi… ## $ chromosome_nr \u0026lt;dbl\u0026gt; 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2… ## $ status \u0026lt;chr\u0026gt; \u0026quot;available\u0026quot;, \u0026quot;available\u0026quot;, \u0026quot;available\u0026quot;, \u0026quot;ava… ## $ vector \u0026lt;chr\u0026gt; \u0026quot;FBgn0036461\u0026quot;, \u0026quot;FBgn0001104\u0026quot;, \u0026quot;FBgn0010869\\… ## $ landing_site \u0026lt;chr\u0026gt; \u0026quot;dZip71B\\ncg10006\\nZip71B\\nZinc/iron regula… ## $ primary_PI \u0026lt;dbl\u0026gt; 0.4700000, 0.4900000, 0.3900000, 0.4398482,… ## $ primary_SEM \u0026lt;dbl\u0026gt; 0.03000000, 0.08100000, 0.04800000, 0.07489… ## $ date \u0026lt;date\u0026gt; 2011-07-08, 2012-10-02, 2011-06-09, 2011-0… ## $ physical_abnormality \u0026lt;chr\u0026gt; \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;+\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;+\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;… We can collect the entire data cleaning code into a single script that we can source whenever we need to re-create the table.\nlibrary(dplyr) library(tidyr) library(readxl) library(stringr) library(biomaRt) suppl\u0026lt;-readxl::read_excel(\u0026quot;/path/to/directory/Walkinshaw2015_suppl.xlsx\u0026quot;) # copy text formatted dates to a separate table suppl_date \u0026lt;- suppl %\u0026gt;% filter(str_detect(DATE,\u0026quot;[0-9]/\u0026quot;)) # change 4 digit years to 2 digit years using a regular expression with gsub suppl_date$DATE \u0026lt;- gsub(\u0026quot;(20)([0-9][0-9])\u0026quot;, \u0026quot;\\\\2\u0026quot;, as.character(suppl_date$DATE)) # remove the text \u0026quot;Date:\u0026quot; and convert to a date object suppl_date \u0026lt;- suppl_date %\u0026gt;% mutate(DATE=str_replace(DATE, \u0026quot;Date:\u0026quot;,\u0026quot;\u0026quot;)) %\u0026gt;% mutate(DATE=as.Date(DATE, format = \u0026quot;%m/%d/%y\u0026quot;)) suppl \u0026lt;- suppl %\u0026gt;% # replace missing dates with NA mutate(DATE=replace_na(DATE,\u0026quot;NA\u0026quot;)) # filter out dates encoded as text filter(!str_detect(DATE, \u0026quot;[0-9]/\u0026quot;)) %\u0026gt;% # convert Excel dates into R dates mutate(DATE=as.Date(as.integer(DATE), origin = \u0026quot;1899-12-30\u0026quot;)) %\u0026gt;% bind_rows(suppl_date) %\u0026gt;% rename(vdrc_id=1, primary_PI=2, primary_SEM=3, date=4, physical_abnormality=5) %\u0026gt;% # mutate(primary_PI = round(primary_PI, 2)) %\u0026gt;% # mutate(primary_SEM = abs(round(primary_SEM, 2))) %\u0026gt;% mutate(physical_abnormality = replace(physical_abnormality, physical_abnormality == \u0026quot;Present\u0026quot;, \u0026quot;+\u0026quot;)) %\u0026gt;% mutate(physical_abnormality = replace(physical_abnormality, is.na(physical_abnormality), \u0026quot;-\u0026quot;)) %\u0026gt;% arrange(vdrc_id) VDRC \u0026lt;- read_xls(\u0026quot;/path/to/directory/REPORT_VdrcCatalogue.xls\u0026quot;, col_types = c(\u0026quot;guess\u0026quot;, \u0026quot;guess\u0026quot;, \u0026quot;guess\u0026quot;, \u0026quot;guess\u0026quot;, \u0026quot;guess\u0026quot;,\u0026quot;guess\u0026quot;,\u0026quot;guess\u0026quot;,\u0026quot;guess\u0026quot;,\u0026quot;guess\u0026quot;, \u0026quot;guess\u0026quot;,\u0026quot;guess\u0026quot;,\u0026quot;skip\u0026quot;,\u0026quot;skip\u0026quot;,\u0026quot;guess\u0026quot;,\u0026quot;guess\u0026quot;)) all_lines\u0026lt;-VDRC %\u0026gt;% dplyr::select(1:11) %\u0026gt;% dplyr::rename(vdrc_id=1,construct_id=2,lib=3,cg_number=4,on_targets=5, off_targets=6,s19=7,can_repeats=8,viability=9,chromosome_nr=10, status=11) %\u0026gt;% dplyr::inner_join(suppl, by=\u0026quot;vdrc_id\u0026quot;) %\u0026gt;% mutate(all_lines, cg_number = ifelse(cg_number == \u0026quot;CG40378\u0026quot;, \u0026quot;CG45781\u0026quot;, cg_number)) %\u0026gt;% mutate(all_lines, cg_number = ifelse(vdrc_id == 104930, \u0026quot;CR13130\u0026quot;, cg_number)) %\u0026gt;% filter(cg_number != \u0026quot;CGnone\u0026quot;) # connect to the ensembl Drosophila melanogaster database v.90 ensembl = useEnsembl(biomart=\u0026quot;ensembl\u0026quot;, dataset=\u0026quot;dmelanogaster_gene_ensembl\u0026quot;, version=90) # get gene names, entrez gene id, FlyBase gene id, and FlyBase CG number # use the cg_number column from all_lines for identifiers genenames \u0026lt;- getBM(attributes=c(\u0026#39;external_gene_name\u0026#39;,\u0026#39;flybase_gene_id\u0026#39;, \u0026#39;flybasecgid_gene\u0026#39;), filters = \u0026#39;flybasecgid_gene\u0026#39;, values = unique(all_lines$cg_number), mart = ensembl) all_lines \u0026lt;- inner_join(genenames, all_lines, by=c(\u0026quot;flybasecgid_gene\u0026quot;=\u0026quot;cg_number\u0026quot;)) %\u0026gt;% dplyr::select(4,1,2,3,5:19) %\u0026gt;% rename(cg_number=flybasecgid_gene) sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS 10.14.2 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] bindrcpp_0.2.2 stringr_1.3.1 readxl_1.2.0 tidyr_0.8.1 ## [5] dplyr_0.7.6 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_0.12.18 knitr_1.20 bindr_0.1.1 magrittr_1.5 ## [5] tidyselect_0.2.4 R6_2.2.2 rlang_0.3.0.1 fansi_0.4.0 ## [9] tools_3.5.1 xfun_0.3 utf8_1.1.4 cli_1.0.1 ## [13] htmltools_0.3.6 yaml_2.2.0 rprojroot_1.3-2 digest_0.6.16 ## [17] assertthat_0.2.0 tibble_2.0.0 crayon_1.3.4 bookdown_0.7 ## [21] purrr_0.2.5 glue_1.3.0 evaluate_0.11 rmarkdown_1.10 ## [25] blogdown_0.8 stringi_1.2.4 cellranger_1.1.0 compiler_3.5.1 ## [29] pillar_1.3.1 backports_1.1.2 pkgconfig_2.0.2 References Stat454 Variance Explained Stack Overflow\n ",
    "date": 1546819200,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1546819200,
    "objectID": "1ded6990ee37098ed069f2acd5201646",
    "permalink": "/post/cleaning-the-walkinshaw-dataset-part-2/",
    "publishdate": "2019-01-07T00:00:00Z",
    "relpermalink": "/post/cleaning-the-walkinshaw-dataset-part-2/",
    "section": "post",
    "summary": "In the last post I scraped data from the Walkinshaw et al. (2015) paper and re-formatted it. In this post I will clean the table with scores from the entire screen provided in the supplemental data. This includes data from lines that didn’t show a significant difference from wildtype. The benefit of having these scores is that one can get a good idea of experimental variance and it would be great if other groups would follow this lead.",
    "tags": [],
    "title": "Cleaning the Walkinshaw dataset. Part 2 ",
    "type": "post"
  },
  {
    "authors": null,
    "categories": [],
    "content": " In 2015, the Davis lab published a genome-wide screen of genes involved in memory using 3207 RNAi lines from the VDRC collection (Walkinshaw et al. 2015 Genetics 199(4):1173-1182). They generously published the their entire primary dataset including the scores for the lines that didn’t show an effect as well as those that did. The data is split across several tables in the paper itself as well as in the supplementary data and some of the identifiers are not standard. In this post I will document how I went about scraping the tables from the PDF of the paper and merging it with the supplementary data to generate a single table with programmatically updated identifiers. I have also provided the data cleaning script as a standalone text file. However, I don’t make any guarantees as to the correctness of the final product and so you should use it with an appropriate level of caution.\nThe paper itself is available for free download from the Genetics website. I saved my copy as Walkinshaw2015.pdf in a directory called Walkinshaw_data in my project directory but you can store it anywhere that is convenient and just remember to write out the appropriate paths. From this document we will scrape data from two tables: 1. Table 2 contains genes whose disruption leads to an enhancement in the memory score AKA performance index (PI) and which were confirmed in a secondary screen, 2. Supplementary Table S2 lists the genes whose knock-down caused a decrease in the PI. The tabulizer package from rOpenSci makes it very easy to perform this task by providing a convenient API to the Java Tabula package for extracting tables from PDFs.\nThe following code snippet extracts the table on page 5 of the PDF.\nlibrary(tabulizer) increasedPI \u0026lt;- tabulizer::extract_tables(\u0026quot;Walkinshaw_data/Walkinshaw2015.pdf\u0026quot;, pages=5)  The output of the extract_tables() command is a list of character matrices where each matrix contains the data from one page of the table. Since Table 2 is entirely contained within one page we can check its formatting by printing the first few rows of the first (and only) element of the list.\nhead(increasedPI[[1]], 10) ## [,1] [,2] [,3] ## [1,] \u0026quot;Transformant\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ## [2,] \u0026quot;ID\u0026quot; \u0026quot;CG no.\u0026quot; \u0026quot;Drosophila gene\u0026quot; ## [3,] \u0026quot;100151\u0026quot; \u0026quot;CG10483\u0026quot; \u0026quot;CG10483\u0026quot; ## [4,] \u0026quot;100363\u0026quot; \u0026quot;CG42614\u0026quot; \u0026quot;scribble\u0026quot; ## [5,] \u0026quot;100624\u0026quot; \u0026quot;CG13521\u0026quot; \u0026quot;roundabout\u0026quot; ## [6,] \u0026quot;100706\u0026quot; \u0026quot;CG1470\u0026quot; \u0026quot;Guanylyl cyclase b-subunit at 100B\u0026quot; ## [7,] \u0026quot;100721\u0026quot; \u0026quot;CG11326\u0026quot; \u0026quot;Thrombospondin\u0026quot; ## [8,] \u0026quot;100727\u0026quot; \u0026quot;CG8715\u0026quot; \u0026quot;lingerer\u0026quot; ## [9,] \u0026quot;101189\u0026quot; \u0026quot;CG42244\u0026quot; \u0026quot;Octb3R\u0026quot; ## [10,] \u0026quot;102058\u0026quot; \u0026quot;CG1128\u0026quot; \u0026quot;a-Esterase-9\u0026quot; ## [,4] [,5] [,6] [,7] ## [1,] \u0026quot;Primary\u0026quot; \u0026quot;Secondary\u0026quot; \u0026quot;Physical\u0026quot; \u0026quot;Mean activity\u0026quot; ## [2,] \u0026quot;PI and SEM line\u0026quot; \u0026quot;PI and SEM line\u0026quot; \u0026quot;abnormality\u0026quot; \u0026quot;difference\u0026quot; ## [3,] \u0026quot;0.64 6 0.10\u0026quot; \u0026quot;0.62 6 0.03\u0026quot; \u0026quot;—\u0026quot; \u0026quot;211.66\u0026quot; ## [4,] \u0026quot;0.85 6 0.04\u0026quot; \u0026quot;0.63 6 0.04\u0026quot; \u0026quot;—\u0026quot; \u0026quot;7.40\u0026quot; ## [5,] \u0026quot;0.55 6 0.06\u0026quot; \u0026quot;0.61 6 0.06\u0026quot; \u0026quot;—\u0026quot; \u0026quot;23.13\u0026quot; ## [6,] \u0026quot;0.62 6 0.07\u0026quot; \u0026quot;0.61 6 0.02\u0026quot; \u0026quot;—\u0026quot; \u0026quot;24.07\u0026quot; ## [7,] \u0026quot;0.65 6 0.07\u0026quot; \u0026quot;0.53 6 0.10\u0026quot; \u0026quot;—\u0026quot; \u0026quot;4.62\u0026quot; ## [8,] \u0026quot;0.65 6 0.12\u0026quot; \u0026quot;0.54 6 0.03\u0026quot; \u0026quot;—\u0026quot; \u0026quot;29.81\u0026quot; ## [9,] \u0026quot;0.67 6 0.03\u0026quot; \u0026quot;0.6 6 0.08\u0026quot; \u0026quot;—\u0026quot; \u0026quot;20.38\u0026quot; ## [10,] \u0026quot;0.65 6 0.04\u0026quot; \u0026quot;0.60 6 0.04\u0026quot; \u0026quot;—\u0026quot; \u0026quot;2.12\u0026quot; ## [,8] ## [1,] \u0026quot;Act.\u0026quot; ## [2,] \u0026quot;sig.\u0026quot; ## [3,] \u0026quot;\u0026quot; ## [4,] \u0026quot;\u0026quot; ## [5,] \u0026quot;\u0026quot; ## [6,] \u0026quot;\u0026quot; ## [7,] \u0026quot;\u0026quot; ## [8,] \u0026quot;***\u0026quot; ## [9,] \u0026quot;\u0026quot; ## [10,] \u0026quot;\u0026quot; There are a few things that are immediately obvious that need to be corrected:\nInformation that should be restricted to one row is sometimes spread out over two rows. For example the column names are in rows 1 and 2. Full gene names are used instead of standardized gene symbols. The symbol for +/- is somehow converted to the number 6. The PI and SEM should be in separate columns. The PI and SEM are coded as character data types and should be numeric. Absence of physical abnormality is coded by a non-standard character instead of “-”.  We will wait to correct these issues until we have added the data for lines that showed a decrease in the memory score from Supplemental Table S2. In the meantime we will convert this table to a dataframe and add a column that indicates the sign of the change in memory relative to wildtype.\nlibrary(dplyr) library(tidyr) df_increasedPI \u0026lt;- # convert to dataframe as_data_frame(increasedPI[[1]]) %\u0026gt;% # Remove rows containing column names slice(-1:-2) %\u0026gt;% # Add proper column names rename(vdrc_id=1, cg_number=2, gene_name=3, primary_score=4, secondary_score=5, physical_abnormality=6, mean_activity_difference=7, act_sig=8) %\u0026gt;% mutate(change_in_memory = \u0026quot;+\u0026quot;) Next we repeat this process with Supplementary Table S2 which contains the data for lines where disrupting the gene product causes a reduction in the memory score or PI. We again use the tabulizer package to read in the data but this time it is spread over several pages (pp. 13-38) of the Extended PDF. Looking at the first 10 rows of the first matrix of the output we notice that there are 18 columns, some of which have no elements.\ndecreasedPI \u0026lt;- tabulizer::extract_tables(\u0026quot;Walkinshaw_data/Walkinshaw2015.pdf\u0026quot;, pages=13:38) head(decreasedPI[[1]], 10) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] \u0026quot;TRANSFORMANT ID\u0026quot; \u0026quot;\u0026quot; \u0026quot;CG NUMBER\u0026quot; \u0026quot;\u0026quot; \u0026quot;DROSOPHILA GENE\u0026quot; \u0026quot;\u0026quot; ## [2,] \u0026quot;11471\u0026quot; \u0026quot;\u0026quot; \u0026quot;CG33517\u0026quot; \u0026quot;\u0026quot; \u0026quot;Dopamine 2-like\u0026quot; \u0026quot;\u0026quot; ## [3,] \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;receptor\u0026quot; \u0026quot;\u0026quot; ## [4,] \u0026quot;11817\u0026quot; \u0026quot;\u0026quot; \u0026quot;CG42260\u0026quot; \u0026quot;\u0026quot; \u0026quot;CG42260\u0026quot; \u0026quot;\u0026quot; ## [5,] \u0026quot;13140\u0026quot; \u0026quot;\u0026quot; \u0026quot;CG31546\u0026quot; \u0026quot;\u0026quot; \u0026quot;CG31546\u0026quot; \u0026quot;\u0026quot; ## [6,] \u0026quot;19124\u0026quot; \u0026quot;\u0026quot; \u0026quot;CG2204\u0026quot; \u0026quot;\u0026quot; \u0026quot;G protein oα 47A\u0026quot; \u0026quot;\u0026quot; ## [7,] \u0026quot;26876\u0026quot; \u0026quot;\u0026quot; \u0026quot;CG7485\u0026quot; \u0026quot;\u0026quot; \u0026quot;Octopamine-\u0026quot; \u0026quot;\u0026quot; ## [8,] \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;Tyramine receptor\u0026quot; \u0026quot;\u0026quot; ## [9,] \u0026quot;37549\u0026quot; \u0026quot;\u0026quot; \u0026quot;CG6711\u0026quot; \u0026quot;\u0026quot; \u0026quot;TBP-associated\u0026quot; \u0026quot;\u0026quot; ## [10,] \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;factor 2\u0026quot; \u0026quot;\u0026quot; ## [,7] [,8] [,9] [,10] [,11] ## [1,] \u0026quot;PRIMARY PI \u0026amp; SEM LINE\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;SECONDARYPI \u0026amp; SEM LINE\u0026quot; \u0026quot;\u0026quot; ## [2,] \u0026quot;0.16 ± 0.09\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;0.13 ± 0.10\u0026quot; \u0026quot;\u0026quot; ## [3,] \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ## [4,] \u0026quot;0.05 ± 0.16\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;0.15 ± 0.13\u0026quot; \u0026quot;\u0026quot; ## [5,] \u0026quot;0.21 ± 0.15\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;0.15 ± 0.04\u0026quot; \u0026quot;\u0026quot; ## [6,] \u0026quot;-0.12 ± 0.08\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;0.15 ± 0.06\u0026quot; \u0026quot;\u0026quot; ## [7,] \u0026quot;0.25 ± 0.07\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;0.19 ± 0.14\u0026quot; \u0026quot;\u0026quot; ## [8,] \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ## [9,] \u0026quot;0.15 ± 0.05\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;0.24 ± 0.14\u0026quot; \u0026quot;\u0026quot; ## [10,] \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ## [,12] [,13] [,14] [,15] [,16] [,17] ## [1,] \u0026quot;\u0026quot; \u0026quot;PHYSICAL ABNORM.\u0026quot; \u0026quot;\u0026quot; \u0026quot;MEAN ACTIVITY DIFF.\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ## [2,] \u0026quot;\u0026quot; \u0026quot;-\u0026quot; \u0026quot;\u0026quot; \u0026quot;-5.63\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ## [3,] \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ## [4,] \u0026quot;\u0026quot; \u0026quot;+\u0026quot; \u0026quot;\u0026quot; \u0026quot;NT\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ## [5,] \u0026quot;\u0026quot; \u0026quot;-\u0026quot; \u0026quot;\u0026quot; \u0026quot;NT\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ## [6,] \u0026quot;\u0026quot; \u0026quot;+\u0026quot; \u0026quot;\u0026quot; \u0026quot;NT\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ## [7,] \u0026quot;\u0026quot; \u0026quot;-\u0026quot; \u0026quot;\u0026quot; \u0026quot;NT\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ## [8,] \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ## [9,] \u0026quot;\u0026quot; \u0026quot;-\u0026quot; \u0026quot;\u0026quot; \u0026quot;NT\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ## [10,] \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ## [,18] ## [1,] \u0026quot;ACT. SIG.\u0026quot; ## [2,] \u0026quot;\u0026quot; ## [3,] \u0026quot;\u0026quot; ## [4,] \u0026quot;\u0026quot; ## [5,] \u0026quot;\u0026quot; ## [6,] \u0026quot;\u0026quot; ## [7,] \u0026quot;\u0026quot; ## [8,] \u0026quot;\u0026quot; ## [9,] \u0026quot;\u0026quot; ## [10,] \u0026quot;\u0026quot; We want to make sure that all of the matrices of the output list are uniform so we use sapply() with the dim() function to check the dimensions of each of the 26 matrices in the list. Each column in the output below encodes the data from a single matrix of the list with the row and column dimensions encoded in rows 1 and 2 respectively.\nsapply(decreasedPI, dim) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] ## [1,] 18 26 28 28 28 28 27 28 27 27 27 27 28 ## [2,] 18 8 8 8 8 8 8 8 8 8 8 8 8 ## [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] ## [1,] 28 28 27 28 28 26 27 28 26 28 28 ## [2,] 8 8 8 8 8 8 8 8 8 8 8 ## [,25] [,26] ## [1,] 27 24 ## [2,] 8 8 From this output table we see that the first matrix has 18 columns but each of the other matrices have 8 columns each. We will define a custom function, dataframe_from_list(), that removes empty columns from each matrix using the remove_empty_cols() function of the janitor package, converts missing data to NAs, and renames the columns.\nlibrary(janitor) dataframe_from_list\u0026lt;-function(mylist){ data.frame(mylist, stringsAsFactors = FALSE) %\u0026gt;% mutate_all(funs(na_if(.,\u0026quot;\u0026quot;))) %\u0026gt;% remove_empty(\u0026quot;cols\u0026quot;) %\u0026gt;% rename(vdrc_id=1, cg_number=2, gene_name=3, primary_score=4, secondary_score=5, physical_abnormality=6, mean_activity_difference=7,act_sig=8) } Now we use lapply() to apply this function to each element of the decreasedPI list of matrices and convert it to a list of dataframes. We will convert the list to a single dataframe with the bind_rows() function from the tidyverse dplyr package and add a new column called change_in_memory with value “-” to indicate a decrease in memory.\ndf_decreasedPI\u0026lt;-lapply(decreasedPI, dataframe_from_list) df_decreasedPI\u0026lt;-df_decreasedPI %\u0026gt;% bind_rows() %\u0026gt;% # Remove the first row containing column names slice(-1) %\u0026gt;% # Add column to indicate significant reduction in memory score mutate(change_in_memory = \u0026quot;-\u0026quot;) Unlike the increasedPI table not all cases in the decreasedPI table have scores for changes in activity which suggests that this test wasn’t done for all lines. In our dataframe we have NAs in the act_sig column in all cases where there isn’t a significant difference. In order to match the df_increasedPI table we want to change the act_sig column to have a null value in all cases that have a value in the mean_activity_difference column rather than an NA.\ndf_decreasedPI\u0026lt;-df_decreasedPI %\u0026gt;% mutate(act_sig=if_else(!is.na(mean_activity_difference), \u0026quot;\u0026quot;, act_sig)) head(df_decreasedPI, 10) ## vdrc_id cg_number gene_name primary_score secondary_score ## 1 11471 CG33517 Dopamine 2-like 0.16 ± 0.09 0.13 ± 0.10 ## 2 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; receptor \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 3 11817 CG42260 CG42260 0.05 ± 0.16 0.15 ± 0.13 ## 4 13140 CG31546 CG31546 0.21 ± 0.15 0.15 ± 0.04 ## 5 19124 CG2204 G protein oα 47A -0.12 ± 0.08 0.15 ± 0.06 ## 6 26876 CG7485 Octopamine- 0.25 ± 0.07 0.19 ± 0.14 ## 7 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; Tyramine receptor \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 8 37549 CG6711 TBP-associated 0.15 ± 0.05 0.24 ± 0.14 ## 9 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; factor 2 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; ## 10 46757 CG3977 Copper transporter 0.23 ± 0.04 0.18 ± 0.03 ## physical_abnormality mean_activity_difference act_sig change_in_memory ## 1 - -5.63 - ## 2 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; - ## 3 + NT - ## 4 - NT - ## 5 + NT - ## 6 - NT - ## 7 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; - ## 8 - NT - ## 9 \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; \u0026lt;NA\u0026gt; - ## 10 - 11.51 - We can now combine the two dataframes for increased and decreased scores into a single one that we will call significant_lines.\nsignificant_lines \u0026lt;- bind_rows(df_decreasedPI, df_increasedPI) glimpse(significant_lines) ## Observations: 743 ## Variables: 9 ## $ vdrc_id \u0026lt;chr\u0026gt; \u0026quot;11471\u0026quot;, NA, \u0026quot;11817\u0026quot;, \u0026quot;13140\u0026quot;, \u0026quot;19124\u0026quot;,… ## $ cg_number \u0026lt;chr\u0026gt; \u0026quot;CG33517\u0026quot;, NA, \u0026quot;CG42260\u0026quot;, \u0026quot;CG31546\u0026quot;, \u0026quot;C… ## $ gene_name \u0026lt;chr\u0026gt; \u0026quot;Dopamine 2-like\u0026quot;, \u0026quot;receptor\u0026quot;, \u0026quot;CG42260… ## $ primary_score \u0026lt;chr\u0026gt; \u0026quot;0.16 ± 0.09\u0026quot;, NA, \u0026quot;0.05 ± 0.16\u0026quot;, \u0026quot;0.21… ## $ secondary_score \u0026lt;chr\u0026gt; \u0026quot;0.13 ± 0.10\u0026quot;, NA, \u0026quot;0.15 ± 0.13\u0026quot;, \u0026quot;0.15… ## $ physical_abnormality \u0026lt;chr\u0026gt; \u0026quot;-\u0026quot;, NA, \u0026quot;+\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;+\u0026quot;, \u0026quot;-\u0026quot;, NA, \u0026quot;-\u0026quot;, N… ## $ mean_activity_difference \u0026lt;chr\u0026gt; \u0026quot;-5.63\u0026quot;, NA, \u0026quot;NT\u0026quot;, \u0026quot;NT\u0026quot;, \u0026quot;NT\u0026quot;, \u0026quot;NT\u0026quot;, NA… ## $ act_sig \u0026lt;chr\u0026gt; \u0026quot;\u0026quot;, NA, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, NA, \u0026quot;\u0026quot;, NA, \u0026quot;\u0026quot;,… ## $ change_in_memory \u0026lt;chr\u0026gt; \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;,… In the next chunk of code we will correct all of these problems using packages from the tidyverse. We will use the pipe operator %\u0026gt;% from the tidyr package to string together operations. What each element of the code does is commented in the code chunk below so I won’t break it down further here. For the time being we will remove the gene names from the table and add in standardized symbols from a public database in the next post.\nsignificant_lines \u0026lt;- significant_lines %\u0026gt;% # Put PI and SEM in separate columns separate(primary_score, c(\u0026quot;primary_PI\u0026quot;, \u0026quot;dummy\u0026quot;, \u0026quot;primary_SEM\u0026quot;), \u0026quot; \u0026quot;)%\u0026gt;% dplyr::select(-dummy) %\u0026gt;% separate(secondary_score, c(\u0026quot;secondary_PI\u0026quot;, \u0026quot;dummy\u0026quot;, \u0026quot;secondary_SEM\u0026quot;), \u0026quot; \u0026quot;) %\u0026gt;% # remove +/- character and gene names dplyr::select(-dummy, -gene_name) %\u0026gt;% # Remove all rows that don\u0026#39;t have a valid vdrc_id filter(vdrc_id != \u0026quot;\u0026quot;) %\u0026gt;% # Encode physical abnormality by + or - mutate(physical_abnormality = replace(physical_abnormality, physical_abnormality != \u0026quot;+\u0026quot;, \u0026quot;-\u0026quot;)) %\u0026gt;% # convert VDRC identifiers to integers mutate(vdrc_id = strtoi(vdrc_id)) %\u0026gt;% # Convert PI and SEM to numeric mutate(primary_PI = as.double(primary_PI), primary_SEM = abs(as.double(primary_SEM))) %\u0026gt;% mutate(secondary_PI = as.double(secondary_PI), secondary_SEM = abs(as.double(secondary_SEM))) %\u0026gt;% # Convert Mean Activity Difference to numeric and add column to indicate increase in memory mutate(mean_activity_difference = as.double(mean_activity_difference)) %\u0026gt;% # Sort the dataframe by the VDRC identifier arrange(vdrc_id) Now if we inspect the dataframe we see data for 599 lines and the scores are separated into separate columns.\nglimpse(significant_lines) ## Observations: 599 ## Variables: 10 ## $ vdrc_id \u0026lt;int\u0026gt; 11471, 11817, 13140, 19124, 26876, 3754… ## $ cg_number \u0026lt;chr\u0026gt; \u0026quot;CG33517\u0026quot;, \u0026quot;CG42260\u0026quot;, \u0026quot;CG31546\u0026quot;, \u0026quot;CG220… ## $ primary_PI \u0026lt;dbl\u0026gt; 0.16, 0.05, 0.21, -0.12, 0.25, 0.15, 0.… ## $ primary_SEM \u0026lt;dbl\u0026gt; 0.09, 0.16, 0.15, 0.08, 0.07, 0.05, 0.0… ## $ secondary_PI \u0026lt;dbl\u0026gt; 0.13, 0.15, 0.15, 0.15, 0.19, 0.24, 0.1… ## $ secondary_SEM \u0026lt;dbl\u0026gt; 0.10, 0.13, 0.04, 0.06, 0.14, 0.14, 0.0… ## $ physical_abnormality \u0026lt;chr\u0026gt; \u0026quot;-\u0026quot;, \u0026quot;+\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;+\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;,… ## $ mean_activity_difference \u0026lt;dbl\u0026gt; -5.63, NA, NA, NA, NA, NA, 11.51, NA, 2… ## $ act_sig \u0026lt;chr\u0026gt; \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;,… ## $ change_in_memory \u0026lt;chr\u0026gt; \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;,… We can do some basic checks to see if our cleaned data matches the data in the publication. The paper indicates that 600 lines showed a significant change in memory scores which corresponds quite well with the 599 rows in our cleaned dataframe. Of the 600 lines 42 lines showed an increased PI. We can filter our dataframe with this information.\nsignificant_lines %\u0026gt;% dplyr::filter(change_in_memory==\u0026quot;+\u0026quot;) %\u0026gt;% glimpse() ## Observations: 42 ## Variables: 10 ## $ vdrc_id \u0026lt;int\u0026gt; 100151, 100363, 100624, 100706, 100721,… ## $ cg_number \u0026lt;chr\u0026gt; \u0026quot;CG10483\u0026quot;, \u0026quot;CG42614\u0026quot;, \u0026quot;CG13521\u0026quot;, \u0026quot;CG147… ## $ primary_PI \u0026lt;dbl\u0026gt; 0.64, 0.85, 0.55, 0.62, 0.65, 0.65, 0.6… ## $ primary_SEM \u0026lt;dbl\u0026gt; 0.10, 0.04, 0.06, 0.07, 0.07, 0.12, 0.0… ## $ secondary_PI \u0026lt;dbl\u0026gt; 0.62, 0.63, 0.61, 0.61, 0.53, 0.54, 0.6… ## $ secondary_SEM \u0026lt;dbl\u0026gt; 0.03, 0.04, 0.06, 0.02, 0.10, 0.03, 0.0… ## $ physical_abnormality \u0026lt;chr\u0026gt; \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;, \u0026quot;-\u0026quot;,… ## $ mean_activity_difference \u0026lt;dbl\u0026gt; 211.66, 7.40, 23.13, 24.07, 4.62, 29.81… ## $ act_sig \u0026lt;chr\u0026gt; \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;***\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;, … ## $ change_in_memory \u0026lt;chr\u0026gt; \u0026quot;+\u0026quot;, \u0026quot;+\u0026quot;, \u0026quot;+\u0026quot;, \u0026quot;+\u0026quot;, \u0026quot;+\u0026quot;, \u0026quot;+\u0026quot;, \u0026quot;+\u0026quot;, \u0026quot;+\u0026quot;,… So our dataframe also has 42 lines that show an increased memory score matching the paper.\nI have collected all of the data cleaning commands from this post in a script that is pasted below. You can source it to build the significant_lines dataframe. In the next post I will describe how to clean the supplemental table that contains the entire dataset of 3207 lines and update identifiers programmatically using the biomaRt package to query the Ensembl BioMart webservice.\nlibrary(tabulizer) library(dplyr) library(tidyr) library(janitor) dataframe_from_list\u0026lt;-function(mylist){ data.frame(mylist, stringsAsFactors = FALSE) %\u0026gt;% mutate_all(funs(na_if(.,\u0026quot;\u0026quot;))) %\u0026gt;% remove_empty(\u0026quot;cols\u0026quot;) %\u0026gt;% rename(vdrc_id=1, cg_number=2, gene_name=3, primary_score=4, secondary_score=5, physical_abnormality=6, mean_activity_difference=7,act_sig=8) } increasedPI \u0026lt;- tabulizer::extract_tables(\u0026quot;/path/to/directory/Walkinshaw2016.pdf\u0026quot;, pages=5) # Extract Table 2 located on page 5 of the manuscript df_increasedPI \u0026lt;- # convert to dataframe as_data_frame(increasedPI[[1]]) %\u0026gt;% # Remove rows containing column names slice(-1:-2) %\u0026gt;% # Add proper column names dataframe_from_list() %\u0026gt;% # Add column to indicate sign of change in memory mutate(change_in_memory = \u0026quot;+\u0026quot;) decreasedPI \u0026lt;- tabulizer::extract_tables(\u0026quot;/path/to/directory//Walkinshaw2016.pdf\u0026quot;, pages=13:38) df_decreasedPI\u0026lt;-lapply(decreasedPI, dataframe_from_list) df_decreasedPI\u0026lt;-df_decreasedPI %\u0026gt;% bind_rows() %\u0026gt;% # Remove the first row containing column names slice(-1) %\u0026gt;% # Add column to indicate significant reduction in memory score mutate(change_in_memory = \u0026quot;-\u0026quot;) %\u0026gt;% mutate(act_sig=if_else(!is.na(mean_activity_difference), \u0026quot;\u0026quot;, act_sig)) significant_lines \u0026lt;- bind_rows(df_decreasedPI, df_increasedPI) %\u0026gt;% # Put PI and SEM in separate columns separate(primary_score, c(\u0026quot;primary_PI\u0026quot;, \u0026quot;dummy\u0026quot;, \u0026quot;primary_SEM\u0026quot;), \u0026quot; \u0026quot;)%\u0026gt;% dplyr::select(-dummy) %\u0026gt;% separate(secondary_score, c(\u0026quot;secondary_PI\u0026quot;, \u0026quot;dummy\u0026quot;, \u0026quot;secondary_SEM\u0026quot;), \u0026quot; \u0026quot;) %\u0026gt;% # remove +/- character and gene names dplyr::select(-dummy, -gene_name) %\u0026gt;% # Remove all rows that don\u0026#39;t have a valid vdrc_id filter(vdrc_id != \u0026quot;\u0026quot;) %\u0026gt;% # Encode physical abnormality by + or - mutate(physical_abnormality = replace(physical_abnormality, physical_abnormality != \u0026quot;+\u0026quot;, \u0026quot;-\u0026quot;)) %\u0026gt;% # convert VDRC identifiers to integers mutate(vdrc_id = strtoi(vdrc_id)) %\u0026gt;% # Convert PI and SEM to numeric mutate(primary_PI = as.double(primary_PI), primary_SEM = abs(as.double(primary_SEM))) %\u0026gt;% mutate(secondary_PI = as.double(secondary_PI), secondary_SEM = abs(as.double(secondary_SEM))) %\u0026gt;% # Convert Mean Activity Difference to numeric and add column to indicate increase in memory mutate(mean_activity_difference = as.double(mean_activity_difference)) %\u0026gt;% # Sort the dataframe by the VDRC identifier arrange(vdrc_id) Session Info sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS 10.14.2 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] janitor_1.1.1 bindrcpp_0.2.2 tidyr_0.8.1 dplyr_0.7.6 ## [5] tabulizer_0.2.2 ## ## loaded via a namespace (and not attached): ## [1] Rcpp_0.12.18 knitr_1.20 bindr_0.1.1 ## [4] magrittr_1.5 tidyselect_0.2.4 R6_2.2.2 ## [7] rlang_0.3.0.1 fansi_0.4.0 stringr_1.3.1 ## [10] tools_3.5.1 xfun_0.3 png_0.1-7 ## [13] utf8_1.1.4 cli_1.0.1 htmltools_0.3.6 ## [16] yaml_2.2.0 rprojroot_1.3-2 digest_0.6.16 ## [19] assertthat_0.2.0 tibble_2.0.0 crayon_1.3.4 ## [22] bookdown_0.7 rJava_0.9-10 purrr_0.2.5 ## [25] glue_1.3.0 evaluate_0.11 rmarkdown_1.10 ## [28] blogdown_0.8 stringi_1.2.4 pillar_1.3.1 ## [31] compiler_3.5.1 tabulizerjars_1.0.1 backports_1.1.2 ## [34] pkgconfig_2.0.2  ",
    "date": 1546300800,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1546300800,
    "objectID": "d63deeb0a94304e46235de15e7a420df",
    "permalink": "/post/cleaning-the-walkinshaw-dataset-part-i/",
    "publishdate": "2019-01-01T00:00:00Z",
    "relpermalink": "/post/cleaning-the-walkinshaw-dataset-part-i/",
    "section": "post",
    "summary": "In 2015, the Davis lab published a genome-wide screen of genes involved in memory using 3207 RNAi lines from the VDRC collection (Walkinshaw et al. 2015 Genetics 199(4):1173-1182). They generously published the their entire primary dataset including the scores for the lines that didn’t show an effect as well as those that did. The data is split across several tables in the paper itself as well as in the supplementary data and some of the identifiers are not standard.",
    "tags": [],
    "title": "Cleaning the Walkinshaw dataset. Part I",
    "type": "post"
  },
  {
    "authors": null,
    "categories": null,
    "content": " In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n",
    "date": 1536476400,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1536476400,
    "objectID": "6a451186c775f5f0adb3a0416d0cb711",
    "permalink": "/tutorial/example/",
    "publishdate": "2018-09-09T00:00:00-07:00",
    "relpermalink": "/tutorial/example/",
    "section": "tutorial",
    "summary": "In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;",
    "tags": null,
    "title": "Example Page",
    "type": "docs"
  },
  {
    "authors": [],
    "categories": null,
    "content": "Click on the Slides button above to view the built-in slides feature.\n Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n",
    "date": 1483257600,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1483257600,
    "objectID": "96344c08df50a1b693cc40432115cbe3",
    "permalink": "/talk/example/",
    "publishdate": "2017-01-01T00:00:00-08:00",
    "relpermalink": "/talk/example/",
    "section": "talk",
    "summary": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam.",
    "tags": [],
    "title": "Example Talk",
    "type": "talk"
  },
  {
    "authors": null,
    "categories": null,
    "content": "",
    "date": 1461740400,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1461740400,
    "objectID": "d1311ddf745551c9e117aa4bb7e28516",
    "permalink": "/project/external-project/",
    "publishdate": "2016-04-27T00:00:00-07:00",
    "relpermalink": "/project/external-project/",
    "section": "project",
    "summary": "An example of linking directly to an external project website using `external_link`.",
    "tags": [
      "Demo"
    ],
    "title": "External Project",
    "type": "project"
  },
  {
    "authors": null,
    "categories": null,
    "content": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n",
    "date": 1461740400,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1461740400,
    "objectID": "8f66d660a9a2edc2d08e68cc30f701f7",
    "permalink": "/project/internal-project/",
    "publishdate": "2016-04-27T00:00:00-07:00",
    "relpermalink": "/project/internal-project/",
    "section": "project",
    "summary": "An example of using the in-built project page.",
    "tags": [
      "Deep Learning"
    ],
    "title": "Internal Project",
    "type": "project"
  },
  {
    "authors": [
      "GA Cushen"
    ],
    "categories": null,
    "content": "More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n",
    "date": 1441090800,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1441090800,
    "objectID": "d77fa4a74076ffcd7ca6c21cfc27a4b2",
    "permalink": "/publication/person-re-id/",
    "publishdate": "2015-09-01T00:00:00-07:00",
    "relpermalink": "/publication/person-re-id/",
    "section": "publication",
    "summary": "Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.",
    "tags": [],
    "title": "A Person Re-Identification System For Mobile Devices",
    "type": "publication"
  },
  {
    "authors": [
      "GA Cushen",
      "MS Nixon"
    ],
    "categories": null,
    "content": "More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n",
    "date": 1372662000,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": 1372662000,
    "objectID": "2b4d919e3cf73dfcd0063c88fe01cb00",
    "permalink": "/publication/clothing-search/",
    "publishdate": "2013-07-01T00:00:00-07:00",
    "relpermalink": "/publication/clothing-search/",
    "section": "publication",
    "summary": "We present a mobile visual clothing search system whereby a smart phone user can either choose a social networking photo or take a new photo of a person wearing clothing of interest and search for similar clothing in a retail database. From the query image, the person is detected, clothing is segmented, and clothing features are extracted and quantized. The information is sent from the phone client to a server, where the feature vector of the query image is used to retrieve similar clothing products from online databases. The phone's GPS location is used to re-rank results by retail store location. State of the art work focuses primarily on the recognition of a diverse range of clothing offline and pays little attention to practical applications. Evaluated on a challenging dataset, the system is relatively fast and achieves promising results.",
    "tags": [],
    "title": "Mobile visual clothing search",
    "type": "publication"
  },
  {
    "authors": null,
    "categories": null,
    "content": " Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n",
    "date": -62135596800,
    "expirydate": -62135596800,
    "kind": "page",
    "lang": "en",
    "lastmod": -62135596800,
    "objectID": "c2915ec5da95791851caafdcba9664af",
    "permalink": "/slides/example-slides/",
    "publishdate": "0001-01-01T00:00:00Z",
    "relpermalink": "/slides/example-slides/",
    "section": "slides",
    "summary": "Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$",
    "tags": null,
    "title": "Slides",
    "type": "slides"
  }
]