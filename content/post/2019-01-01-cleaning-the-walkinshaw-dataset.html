---
title: Cleaning the Walkinshaw dataset
author: Amir Fayyazuddin
date: '2019-01-01'
slug: cleaning-the-walkinshaw-dataset
categories: []
tags: []
image:
  caption: ''
  focal_point: ''
---



<p>The Davis lab published a genome-wide screen of genes involved in memory using 3655 RNAi lines from the VDRC collection (Walkinshaw et al. 2015 Genetics 199(4):1173-1182). They generously published the primary data including the scores for the lines that didn’t show an effect as well as those that did. A small problem is that the data is split across several tables in the paper itself as well as in the supplementary data and in some cases identifiers are missing. In this blog post I will document how I went about generating a single table that contained all of the data from the screen and programmatically updated the identifiers. I have also provided the cleaned data table as a tsv file if you don’t want to go to the trouble of following along. However, I don’t make any guarantees as to the correctness of the final product and so you should use the table I have generated here with an appropriate level of caution.</p>
<p>First we need a PDF copy of paper which you can get the paper from the Genetics website: <a href="http://www.genetics.org/content/genetics/199/4/1173.full.pdf" class="uri">http://www.genetics.org/content/genetics/199/4/1173.full.pdf</a>. I saved my copy as Walkinshaw2016.pdf in my project directory but you can store it anywhere that is convenient and just remember to write out the appropriate paths. From this document we will scrape data from two tables. Table 2 has the list of all genes which when knocked down increased the performance index (PI) in a secondary screen. Table S2 lists the genes whose knock-down caused a decrease in the PI. To get this data we will use the tabulizer package from rOpenSci which provides an API to the Java Tabula package for extracting tables from PDFs. I have saved the PDF as Walkinshaw2016.pdf in the working directory. I’ll treat these two tables separately starting with Table 2. The following code extracts the table which is located on page 5.</p>
<pre class="r"><code>library(tabulizer)
increasedPI &lt;- tabulizer::extract_tables(&quot;~/Documents/R_projects/Walkinshaw/Walkinshaw2016.pdf&quot;, pages=5)</code></pre>
<p>The type data structure output by the Tabulizer package is a list of character matrices where each element of the list is a matrix encoding each table on each page. First let’s get the dimensions of the matrix for Table 2.</p>
<pre class="r"><code>dim(increasedPI[[1]])</code></pre>
<pre><code>## [1] 46  8</code></pre>
<p>We can see that Table 2 is encoded as having 8 columns of 46 rows. Let’s take a look at the first few lines of the this matrix:</p>
<pre class="r"><code>head(increasedPI[[1]])</code></pre>
<pre><code>##      [,1]           [,2]      [,3]                                
## [1,] &quot;Transformant&quot; &quot;&quot;        &quot;&quot;                                  
## [2,] &quot;ID&quot;           &quot;CG no.&quot;  &quot;Drosophila gene&quot;                   
## [3,] &quot;100151&quot;       &quot;CG10483&quot; &quot;CG10483&quot;                           
## [4,] &quot;100363&quot;       &quot;CG42614&quot; &quot;scribble&quot;                          
## [5,] &quot;100624&quot;       &quot;CG13521&quot; &quot;roundabout&quot;                        
## [6,] &quot;100706&quot;       &quot;CG1470&quot;  &quot;Guanylyl cyclase b-subunit at 100B&quot;
##      [,4]              [,5]              [,6]          [,7]           
## [1,] &quot;Primary&quot;         &quot;Secondary&quot;       &quot;Physical&quot;    &quot;Mean activity&quot;
## [2,] &quot;PI and SEM line&quot; &quot;PI and SEM line&quot; &quot;abnormality&quot; &quot;difference&quot;   
## [3,] &quot;0.64 6 0.10&quot;     &quot;0.62 6 0.03&quot;     &quot;—&quot;           &quot;211.66&quot;       
## [4,] &quot;0.85 6 0.04&quot;     &quot;0.63 6 0.04&quot;     &quot;—&quot;           &quot;7.40&quot;         
## [5,] &quot;0.55 6 0.06&quot;     &quot;0.61 6 0.06&quot;     &quot;—&quot;           &quot;23.13&quot;        
## [6,] &quot;0.62 6 0.07&quot;     &quot;0.61 6 0.02&quot;     &quot;—&quot;           &quot;24.07&quot;        
##      [,8]  
## [1,] &quot;Act.&quot;
## [2,] &quot;sig.&quot;
## [3,] &quot;&quot;    
## [4,] &quot;&quot;    
## [5,] &quot;&quot;    
## [6,] &quot;&quot;</code></pre>
<p>In order to wrangle this data we will first convert it into a dataframe and specifying the number of rows in the matrix. Let’s look at the first 10 lines to develop a strategy for retaining all the useful information while removing artifacts of the conversion process.</p>
<pre class="r"><code>df_increasedPI&lt;-data.frame(increasedPI[[1]], stringsAsFactors = FALSE)
head(df_increasedPI, 10)</code></pre>
<pre><code>##              X1      X2                                 X3              X4
## 1  Transformant                                                    Primary
## 2            ID  CG no.                    Drosophila gene PI and SEM line
## 3        100151 CG10483                            CG10483     0.64 6 0.10
## 4        100363 CG42614                           scribble     0.85 6 0.04
## 5        100624 CG13521                         roundabout     0.55 6 0.06
## 6        100706  CG1470 Guanylyl cyclase b-subunit at 100B     0.62 6 0.07
## 7        100721 CG11326                     Thrombospondin     0.65 6 0.07
## 8        100727  CG8715                           lingerer     0.65 6 0.12
## 9        101189 CG42244                             Octb3R     0.67 6 0.03
## 10       102058  CG1128                       a-Esterase-9     0.65 6 0.04
##                 X5          X6            X7   X8
## 1        Secondary    Physical Mean activity Act.
## 2  PI and SEM line abnormality    difference sig.
## 3      0.62 6 0.03           —        211.66     
## 4      0.63 6 0.04           —          7.40     
## 5      0.61 6 0.06           —         23.13     
## 6      0.61 6 0.02           —         24.07     
## 7      0.53 6 0.10           —          4.62     
## 8      0.54 6 0.03           —         29.81  ***
## 9       0.6 6 0.08           —         20.38     
## 10     0.60 6 0.04           —          2.12</code></pre>
<p>There are a few things that are immediately obvious that need to be corrected:</p>
<ol style="list-style-type: decimal">
<li>The columns are named X1, X2, etc while the actual column names are in rows 1 and 2.</li>
<li>Information that should be restricted to one row is sometimes spread out over two rows.</li>
<li>Full gene names are used instead of standardized gene symbols.</li>
<li>The symbol for +/- is somehow converted to the number 6.</li>
<li>The PI and SEM should be in separate columns.</li>
<li>The PI and SEM are coded as character data types and should be numeric.</li>
<li>Absence of physical abnormality is coded by a non-standard character we want to convert to a “-”.</li>
</ol>
<p>In the next chunk of code we will correct all of these problems using packages from the tidyverse. We will use the pipe operator to string together a number of operations to do this. What each element of the code does is commented in the code chunk below so I won’t break it down further here. For the time being we will remove the gene names from the table and add in standardized symbols from a public database later on in the post.</p>
<pre class="r"><code>library(tidyverse)
df_increasedPI&lt;-df_increasedPI %&gt;% 
  # Remove rows containing column names
  slice(-1:-2) %&gt;%
  # Add proper column names
  rename(vdrc_id=X1, cg_number=X2, gene_name=X3, primary_score=X4, secondary_score=X5, physical_abnormality=X6, mean_activity_difference=X7, act_sig=X8) %&gt;%
  # Put PI and SEM in separate columns and remove +/- character and gene names
  separate(primary_score, c(&quot;primary_PI&quot;, &quot;dummy&quot;, &quot;primary_SEM&quot;), &quot; &quot;)%&gt;%
  dplyr::select(-dummy) %&gt;%
  separate(secondary_score, c(&quot;secondary_PI&quot;, &quot;dummy&quot;, &quot;secondary_SEM&quot;), &quot; &quot;) %&gt;%
  dplyr::select(-dummy, -gene_name) %&gt;%
  # Remove all rows that don&#39;t have a valid vdrc_id
  filter(vdrc_id != &quot;&quot;) %&gt;%
  # Encode physical abnormality by + or -
  mutate(physical_abnormality = replace(physical_abnormality, physical_abnormality != &quot;+&quot;, &quot;-&quot;)) %&gt;%
  mutate(vdrc_id = as.integer(vdrc_id)) %&gt;%
  # Convert PI and SEM to numeric
  mutate(primary_PI = as.double(primary_PI), primary_SEM = abs(as.double(primary_SEM))) %&gt;%
  mutate(secondary_PI = as.double(secondary_PI), secondary_SEM = abs(as.double(secondary_SEM))) %&gt;%
  # Convert Mean Activity Difference to numeric and add column to indicate increase in memory
  mutate(mean_activity_difference = as.double(mean_activity_difference)) %&gt;%
  mutate(change_in_memory = &quot;+&quot;)</code></pre>
<pre><code>## Warning: Expected 3 pieces. Missing pieces filled with `NA` in 2 rows [31,
## 39].

## Warning: Expected 3 pieces. Missing pieces filled with `NA` in 2 rows [31,
## 39].</code></pre>
<pre class="r"><code>head(df_increasedPI, 10)</code></pre>
<pre><code>##    vdrc_id cg_number primary_PI primary_SEM secondary_PI secondary_SEM
## 1   100151   CG10483       0.64        0.10         0.62          0.03
## 2   100363   CG42614       0.85        0.04         0.63          0.04
## 3   100624   CG13521       0.55        0.06         0.61          0.06
## 4   100706    CG1470       0.62        0.07         0.61          0.02
## 5   100721   CG11326       0.65        0.07         0.53          0.10
## 6   100727    CG8715       0.65        0.12         0.54          0.03
## 7   101189   CG42244       0.67        0.03         0.60          0.08
## 8   102058    CG1128       0.65        0.04         0.60          0.04
## 9   102373    CG3217       0.66        0.12         0.60          0.02
## 10  102563   CR17025       0.56        0.10         0.70          0.05
##    physical_abnormality mean_activity_difference act_sig change_in_memory
## 1                     -                   211.66                        +
## 2                     -                     7.40                        +
## 3                     -                    23.13                        +
## 4                     -                    24.07                        +
## 5                     -                     4.62                        +
## 6                     -                    29.81     ***                +
## 7                     -                    20.38                        +
## 8                     -                     2.12                        +
## 9                     -                     1.53                        +
## 10                    -                     1.65                        +</code></pre>
<p>The next set of data we will clean is the ones where the PIs are significantly less than the mean PI. This data is provided in the Table S2 of the supplemental data. We will read it in again using functions from the Tabulizer package. This data can be round on pages 13-38 of the Extended PDF of this paper. Let’s load in the data and look at the first page</p>
<pre class="r"><code>decreasedPI &lt;- tabulizer::extract_tables(&quot;~/Documents/R_projects/Walkinshaw/Walkinshaw2016.pdf&quot;, pages=13:38)
head(decreasedPI[[1]])</code></pre>
<pre><code>##      [,1]               [,2] [,3]        [,4] [,5]               [,6]
## [1,] &quot;TRANSFORMANT  ID&quot; &quot;&quot;   &quot;CG NUMBER&quot; &quot;&quot;   &quot;DROSOPHILA GENE&quot;  &quot;&quot;  
## [2,] &quot;11471&quot;            &quot;&quot;   &quot;CG33517&quot;   &quot;&quot;   &quot;Dopamine 2-like&quot;  &quot;&quot;  
## [3,] &quot;&quot;                 &quot;&quot;   &quot;&quot;          &quot;&quot;   &quot;receptor&quot;         &quot;&quot;  
## [4,] &quot;11817&quot;            &quot;&quot;   &quot;CG42260&quot;   &quot;&quot;   &quot;CG42260&quot;          &quot;&quot;  
## [5,] &quot;13140&quot;            &quot;&quot;   &quot;CG31546&quot;   &quot;&quot;   &quot;CG31546&quot;          &quot;&quot;  
## [6,] &quot;19124&quot;            &quot;&quot;   &quot;CG2204&quot;    &quot;&quot;   &quot;G protein oα 47A&quot; &quot;&quot;  
##      [,7]                    [,8] [,9] [,10]                    [,11]
## [1,] &quot;PRIMARY PI &amp; SEM LINE&quot; &quot;&quot;   &quot;&quot;   &quot;SECONDARYPI &amp; SEM LINE&quot; &quot;&quot;   
## [2,] &quot;0.16 ± 0.09&quot;           &quot;&quot;   &quot;&quot;   &quot;0.13 ± 0.10&quot;            &quot;&quot;   
## [3,] &quot;&quot;                      &quot;&quot;   &quot;&quot;   &quot;&quot;                       &quot;&quot;   
## [4,] &quot;0.05 ± 0.16&quot;           &quot;&quot;   &quot;&quot;   &quot;0.15 ± 0.13&quot;            &quot;&quot;   
## [5,] &quot;0.21 ± 0.15&quot;           &quot;&quot;   &quot;&quot;   &quot;0.15 ± 0.04&quot;            &quot;&quot;   
## [6,] &quot;-0.12 ± 0.08&quot;          &quot;&quot;   &quot;&quot;   &quot;0.15 ± 0.06&quot;            &quot;&quot;   
##      [,12] [,13]              [,14] [,15]                 [,16] [,17]
## [1,] &quot;&quot;    &quot;PHYSICAL ABNORM.&quot; &quot;&quot;    &quot;MEAN ACTIVITY DIFF.&quot; &quot;&quot;    &quot;&quot;   
## [2,] &quot;&quot;    &quot;-&quot;                &quot;&quot;    &quot;-5.63&quot;               &quot;&quot;    &quot;&quot;   
## [3,] &quot;&quot;    &quot;&quot;                 &quot;&quot;    &quot;&quot;                    &quot;&quot;    &quot;&quot;   
## [4,] &quot;&quot;    &quot;+&quot;                &quot;&quot;    &quot;NT&quot;                  &quot;&quot;    &quot;&quot;   
## [5,] &quot;&quot;    &quot;-&quot;                &quot;&quot;    &quot;NT&quot;                  &quot;&quot;    &quot;&quot;   
## [6,] &quot;&quot;    &quot;+&quot;                &quot;&quot;    &quot;NT&quot;                  &quot;&quot;    &quot;&quot;   
##      [,18]      
## [1,] &quot;ACT. SIG.&quot;
## [2,] &quot;&quot;         
## [3,] &quot;&quot;         
## [4,] &quot;&quot;         
## [5,] &quot;&quot;         
## [6,] &quot;&quot;</code></pre>
<p>As we can see the program thinks there are 18 columns, some of which have no elements. Let’s see if this is the case for every page. We can use the function sapply to get the dimension of the matrix on each page.</p>
<pre class="r"><code>sapply(decreasedPI, dim)</code></pre>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13]
## [1,]   18   26   28   28   28   28   27   28   27    27    27    27    28
## [2,]   18    8    8    8    8    8    8    8    8     8     8     8     8
##      [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24]
## [1,]    28    28    27    28    28    26    27    28    26    28    28
## [2,]     8     8     8     8     8     8     8     8     8     8     8
##      [,25] [,26]
## [1,]    27    24
## [2,]     8     8</code></pre>
<p>Here the columns represent each page in the list while row 1 shows how many rows each matrix has and row 2 shows the number of columns. From this we can tell that the first page is unique in having 18 columns because the rest of them have only 8 columns. However, each page has different numbers of rows. Directly inspecting the table in the PDF we can see that this is indeed the case. To convert this list of matrices into a dataframe like we did for the increased PI we will write a function and apply it to each matrix to end up with a dataframe containing all of the data in the decreasedPI list. This function also removes all empty columns using the remove_empty_cols function of the janitor package and converts all missing data to NAs.</p>
<pre class="r"><code>library(janitor)
dataframe_from_list&lt;-function(mylist){
  data.frame(mylist, stringsAsFactors = FALSE) %&gt;%
    mutate_all(funs(na_if(.,&quot;&quot;))) %&gt;% 
    remove_empty(&quot;cols&quot;)
}</code></pre>
<p>Now we use lapply to apply this function to each element of the decreasedPI list of matrices and convert it to a list of dataframes with the data from this table. We will convert the list to a single dataframe with the bind_rows function from the tidyverse dplyr package and we will rename each of the columns from X1 to X8 so we can re-use the code that we used to clean the increased PI data except now we will code the change_in_memory column with a “-” to indicate a decrease in memory.</p>
<pre class="r"><code>df_decreasedPI&lt;-lapply(decreasedPI, dataframe_from_list)
colnames(df_decreasedPI[[1]])&lt;-c(&quot;X1&quot;,&quot;X2&quot;,&quot;X3&quot;,&quot;X4&quot;,&quot;X5&quot;,&quot;X6&quot;,&quot;X7&quot;,&quot;X8&quot;)
df_decreasedPI&lt;-df_decreasedPI %&gt;%
  bind_rows() %&gt;% 
  # Remove the first row containing column names
  slice(-1) %&gt;%
  rename(vdrc_id=X1, cg_number=X2, gene_name=X3, primary_score=X4, secondary_score=X5, physical_abnormality=X6, mean_activity_difference=X7, act_sig=X8) %&gt;%
  separate(primary_score, c(&quot;primary_PI&quot;, &quot;dummy&quot;, &quot;primary_SEM&quot;), &quot; &quot;) %&gt;%
  dplyr::select(-dummy) %&gt;%
  separate(secondary_score, c(&quot;secondary_PI&quot;, &quot;dummy&quot;, &quot;secondary_SEM&quot;), &quot; &quot;) %&gt;%
  dplyr::select(-dummy, -gene_name) %&gt;%
  filter(vdrc_id != &quot;&quot;) %&gt;% 
  mutate(vdrc_id = as.integer(vdrc_id)) %&gt;%
  mutate(primary_PI = as.double(primary_PI), primary_SEM = abs(as.double(primary_SEM))) %&gt;%
  mutate(secondary_PI = as.double(secondary_PI), secondary_SEM = abs(as.double(secondary_SEM))) %&gt;%
  mutate(mean_activity_difference = as.double(mean_activity_difference))%&gt;%
  mutate(change_in_memory = &quot;-&quot;)</code></pre>
<pre><code>## Warning in evalq(as.double(mean_activity_difference), &lt;environment&gt;): NAs
## introduced by coercion</code></pre>
<pre class="r"><code>head(df_decreasedPI)</code></pre>
<pre><code>##   vdrc_id cg_number primary_PI primary_SEM secondary_PI secondary_SEM
## 1   11471   CG33517       0.16        0.09         0.13          0.10
## 2   11817   CG42260       0.05        0.16         0.15          0.13
## 3   13140   CG31546       0.21        0.15         0.15          0.04
## 4   19124    CG2204      -0.12        0.08         0.15          0.06
## 5   26876    CG7485       0.25        0.07         0.19          0.14
## 6   37549    CG6711       0.15        0.05         0.24          0.14
##   physical_abnormality mean_activity_difference act_sig change_in_memory
## 1                    -                    -5.63    &lt;NA&gt;                -
## 2                    +                       NA    &lt;NA&gt;                -
## 3                    -                       NA    &lt;NA&gt;                -
## 4                    +                       NA    &lt;NA&gt;                -
## 5                    -                       NA    &lt;NA&gt;                -
## 6                    -                       NA    &lt;NA&gt;                -</code></pre>
<p>Unlike the increased PI table not all cases in the decreased PI table has scores for changes in activity which suggests that this test wasn’t done for all lines. In our dataframe we have NAs in the act_sig column in all cases where there isn’t a significant difference. In order to match the df_increasedPI table we want to change the act_sig column to have a null value in all cases that have a value in the mean_activity_difference column rather than an NA</p>
<pre class="r"><code>df_decreasedPI&lt;-df_decreasedPI %&gt;% 
  mutate(act_sig=if_else(!is.na(mean_activity_difference), &quot;&quot;, act_sig))

head(df_decreasedPI, 10)</code></pre>
<pre><code>##    vdrc_id cg_number primary_PI primary_SEM secondary_PI secondary_SEM
## 1    11471   CG33517       0.16        0.09         0.13          0.10
## 2    11817   CG42260       0.05        0.16         0.15          0.13
## 3    13140   CG31546       0.21        0.15         0.15          0.04
## 4    19124    CG2204      -0.12        0.08         0.15          0.06
## 5    26876    CG7485       0.25        0.07         0.19          0.14
## 6    37549    CG6711       0.15        0.05         0.24          0.14
## 7    46757    CG3977       0.23        0.04         0.18          0.03
## 8    48984    CG8451       0.10        0.17         0.22          0.10
## 9   100010    CG7595       0.10        0.12         0.03          0.05
## 10  100029   CG15390       0.17        0.05         0.17          0.05
##    physical_abnormality mean_activity_difference act_sig change_in_memory
## 1                     -                    -5.63                        -
## 2                     +                       NA    &lt;NA&gt;                -
## 3                     -                       NA    &lt;NA&gt;                -
## 4                     +                       NA    &lt;NA&gt;                -
## 5                     -                       NA    &lt;NA&gt;                -
## 6                     -                       NA    &lt;NA&gt;                -
## 7                     -                    11.51                        -
## 8                     -                       NA    &lt;NA&gt;                -
## 9                     -                    25.68                        -
## 10                    -                    22.06                        -</code></pre>
<p>We can now combine the cleaned decreased PI and increased PI datasets. We’ll call this dataframe significant_lines</p>
<pre class="r"><code>significant_lines &lt;- dplyr::full_join(df_increasedPI, df_decreasedPI) %&gt;% 
  arrange(vdrc_id) </code></pre>
<pre><code>## Joining, by = c(&quot;vdrc_id&quot;, &quot;cg_number&quot;, &quot;primary_PI&quot;, &quot;primary_SEM&quot;, &quot;secondary_PI&quot;, &quot;secondary_SEM&quot;, &quot;physical_abnormality&quot;, &quot;mean_activity_difference&quot;, &quot;act_sig&quot;, &quot;change_in_memory&quot;)</code></pre>
<pre class="r"><code>head(significant_lines, 10)</code></pre>
<pre><code>##    vdrc_id cg_number primary_PI primary_SEM secondary_PI secondary_SEM
## 1    11471   CG33517       0.16        0.09         0.13          0.10
## 2    11817   CG42260       0.05        0.16         0.15          0.13
## 3    13140   CG31546       0.21        0.15         0.15          0.04
## 4    19124    CG2204      -0.12        0.08         0.15          0.06
## 5    26876    CG7485       0.25        0.07         0.19          0.14
## 6    37549    CG6711       0.15        0.05         0.24          0.14
## 7    46757    CG3977       0.23        0.04         0.18          0.03
## 8    48984    CG8451       0.10        0.17         0.22          0.10
## 9   100010    CG7595       0.10        0.12         0.03          0.05
## 10  100029   CG15390       0.17        0.05         0.17          0.05
##    physical_abnormality mean_activity_difference act_sig change_in_memory
## 1                     -                    -5.63                        -
## 2                     +                       NA    &lt;NA&gt;                -
## 3                     -                       NA    &lt;NA&gt;                -
## 4                     +                       NA    &lt;NA&gt;                -
## 5                     -                       NA    &lt;NA&gt;                -
## 6                     -                       NA    &lt;NA&gt;                -
## 7                     -                    11.51                        -
## 8                     -                       NA    &lt;NA&gt;                -
## 9                     -                    25.68                        -
## 10                    -                    22.06                        -</code></pre>
<p>One great thing about this paper is that the Davis lab has made available scores from all lines that they tested not just the ones that gave increased or decreased PI scores. These are available in the supplemental data as an excel file (.xlsx). We want to merge this dataset with our significant_lines file. First, however, we need to fill in some missing data. The supplementary file only has the identifier from the RNAi stock center (vdrc_id) so we need to find the corresponding CG number which we will use to fill in the gene names later on. I downloaded the supplementary file into the project directory as Walkinshaw2016_suppl.xlsx. However, I couldn’t find an easy way to get the data from this file into R without miscoding the dates. I converted the file into a csv file before importing it into R using read_csv.</p>
<pre class="r"><code>suppl&lt;-read.csv(&quot;~/Documents/R_projects/Walkinshaw/Walkinshaw2016_suppl.csv&quot;, stringsAsFactors = FALSE)

head(suppl, 10)</code></pre>
<pre><code>##     X   LINE TRIAL    SEM    DATE WING.DEFORMITY.
## 1  NA 105968  0.42 0.0440 6/24/11                
## 2  NA 105691  0.33 0.0570 6/24/11                
## 3  NA 105961  0.22 0.0620 6/24/11                
## 4  NA 105969  0.23 0.0750 6/24/11                
## 5  NA 105970  0.52 0.0410 6/24/11                
## 6  NA 105987  0.11 0.0533 6/24/11                
## 7  NA 107047  0.62 0.0416 6/24/11                
## 8  NA 103951  0.41 0.0231 6/24/11                
## 9  NA 107053  0.40 0.0540 6/24/11                
## 10 NA 107054  0.43 0.0740 6/24/11</code></pre>
<p>Notice that the first column, X, is empty. We will remove this column and change the names of the rest of the columns to match those in the significant_lines file.</p>
<pre class="r"><code>suppl&lt;-suppl %&gt;% 
  dplyr::select(-X)

colnames(suppl)&lt;-c(&quot;vdrc_id&quot;, &quot;primary_PI&quot;, &quot;primary_SEM&quot;, &quot;DATE&quot;, &quot;physical_abnormality&quot;)</code></pre>
<p>While browsing the file I noticed that the DATE column doesn’t have uniform format. Also we want to convert the dates into a format that can be interpreted by R. Let’s look at the full complement of dates in the dataset and to do this we will treat the date as a categorical variable and look at all the levels of the variable.</p>
<pre class="r"><code>levels(as.factor(suppl$DATE))</code></pre>
<pre><code>##   [1] &quot;&quot;                &quot; 6/8/11&quot;         &quot;1/11/11&quot;        
##   [4] &quot;1/11/12&quot;         &quot;1/12/12&quot;         &quot;1/13/12&quot;        
##   [7] &quot;1/17/13&quot;         &quot;1/18/11&quot;         &quot;1/18/12&quot;        
##  [10] &quot;1/18/13&quot;         &quot;1/19/11&quot;         &quot;1/19/12&quot;        
##  [13] &quot;1/20/11&quot;         &quot;1/20/12&quot;         &quot;1/22/13&quot;        
##  [16] &quot;1/24/12&quot;         &quot;1/24/13&quot;         &quot;1/25/12&quot;        
##  [19] &quot;1/25/13&quot;         &quot;1/26/12&quot;         &quot;1/27/12&quot;        
##  [22] &quot;1/29/13&quot;         &quot;1/31/12&quot;         &quot;1/31/13&quot;        
##  [25] &quot;1/5/12&quot;          &quot;1/6/12&quot;          &quot;1/9/00&quot;         
##  [28] &quot;10/10/12&quot;        &quot;10/11/12&quot;        &quot;10/12/11&quot;       
##  [31] &quot;10/12/12&quot;        &quot;10/13/11&quot;        &quot;10/13/12&quot;       
##  [34] &quot;10/14/11&quot;        &quot;10/14/12&quot;        &quot;10/16/12&quot;       
##  [37] &quot;10/17/12&quot;        &quot;10/18/11&quot;        &quot;10/18/12&quot;       
##  [40] &quot;10/19/11&quot;        &quot;10/19/12&quot;        &quot;10/2/12&quot;        
##  [43] &quot;10/20/11&quot;        &quot;10/21/11&quot;        &quot;10/23/06&quot;       
##  [46] &quot;10/23/12&quot;        &quot;10/24/12&quot;        &quot;10/25/12&quot;       
##  [49] &quot;10/26/11&quot;        &quot;10/26/12&quot;        &quot;10/27/11&quot;       
##  [52] &quot;10/28/11&quot;        &quot;10/3/12&quot;         &quot;10/30/12&quot;       
##  [55] &quot;10/31/12&quot;        &quot;10/4/11&quot;         &quot;10/4/12&quot;        
##  [58] &quot;10/5/11&quot;         &quot;10/5/12&quot;         &quot;10/6/11&quot;        
##  [61] &quot;10/7/11&quot;         &quot;10/9/12&quot;         &quot;11/1/11&quot;        
##  [64] &quot;11/1/12&quot;         &quot;11/10/11&quot;        &quot;11/11/11&quot;       
##  [67] &quot;11/15/11&quot;        &quot;11/16/11&quot;        &quot;11/17/11&quot;       
##  [70] &quot;11/18/11&quot;        &quot;11/2/11&quot;         &quot;11/2/12&quot;        
##  [73] &quot;11/29/11&quot;        &quot;11/3/11&quot;         &quot;11/4/11&quot;        
##  [76] &quot;11/9/11&quot;         &quot;12/1/11&quot;         &quot;12/11/12&quot;       
##  [79] &quot;12/12/12&quot;        &quot;12/13/12&quot;        &quot;12/14/11&quot;       
##  [82] &quot;12/15/11&quot;        &quot;12/16/11&quot;        &quot;12/2/11&quot;        
##  [85] &quot;12/21/11&quot;        &quot;12/22/11&quot;        &quot;12/28/11&quot;       
##  [88] &quot;12/29/11&quot;        &quot;2/1/12&quot;          &quot;2/1/13&quot;         
##  [91] &quot;2/12/13&quot;         &quot;2/13/13&quot;         &quot;2/14/12&quot;        
##  [94] &quot;2/14/13&quot;         &quot;2/15/12&quot;         &quot;2/15/13&quot;        
##  [97] &quot;2/16/12&quot;         &quot;2/17/12&quot;         &quot;2/2/12&quot;         
## [100] &quot;2/3/12&quot;          &quot;2/5/13&quot;          &quot;2/6/13&quot;         
## [103] &quot;2/7/12&quot;          &quot;2/7/13&quot;          &quot;2/8/12&quot;         
## [106] &quot;2/8/13&quot;          &quot;2/9/11&quot;          &quot;2/9/12&quot;         
## [109] &quot;3/13/12&quot;         &quot;3/14/12&quot;         &quot;3/15/12&quot;        
## [112] &quot;3/16/12&quot;         &quot;3/2/12&quot;          &quot;3/20/12&quot;        
## [115] &quot;3/20/13&quot;         &quot;3/21/12&quot;         &quot;3/21/13&quot;        
## [118] &quot;3/22/12&quot;         &quot;3/22/13&quot;         &quot;3/23/12&quot;        
## [121] &quot;3/27/12&quot;         &quot;3/28/12&quot;         &quot;3/29/12&quot;        
## [124] &quot;3/30/12&quot;         &quot;3/7/12&quot;          &quot;3/8/12&quot;         
## [127] &quot;3/9/12&quot;          &quot;4/10/12&quot;         &quot;4/11/12&quot;        
## [130] &quot;4/12/12&quot;         &quot;4/13/12&quot;         &quot;4/17/12&quot;        
## [133] &quot;4/18/12&quot;         &quot;4/19/12&quot;         &quot;4/20/12&quot;        
## [136] &quot;4/24/12&quot;         &quot;4/25/12&quot;         &quot;4/26/12&quot;        
## [139] &quot;4/3/12&quot;          &quot;4/4/12&quot;          &quot;4/5/12&quot;         
## [142] &quot;4/6/12&quot;          &quot;5/1/12&quot;          &quot;5/10/12&quot;        
## [145] &quot;5/11/11&quot;         &quot;5/11/12&quot;         &quot;5/12/11&quot;        
## [148] &quot;5/13/11&quot;         &quot;5/15/12&quot;         &quot;5/16/12&quot;        
## [151] &quot;5/17/12&quot;         &quot;5/18/12&quot;         &quot;5/19/11&quot;        
## [154] &quot;5/2/12&quot;          &quot;5/22/12&quot;         &quot;5/23/12&quot;        
## [157] &quot;5/24/12&quot;         &quot;5/25/12&quot;         &quot;5/26/12&quot;        
## [160] &quot;5/3/11&quot;          &quot;5/3/12&quot;          &quot;5/30/12&quot;        
## [163] &quot;5/31/12&quot;         &quot;5/4/12&quot;          &quot;5/5/11&quot;         
## [166] &quot;5/7/13&quot;          &quot;5/8/12&quot;          &quot;5/8/13&quot;         
## [169] &quot;5/9/12&quot;          &quot;6/1/11&quot;          &quot;6/1/12&quot;         
## [172] &quot;6/14/11&quot;         &quot;6/15/11&quot;         &quot;6/16/11&quot;        
## [175] &quot;6/17/11&quot;         &quot;6/21/11&quot;         &quot;6/22/11&quot;        
## [178] &quot;6/23/11&quot;         &quot;6/24/11&quot;         &quot;6/28/11&quot;        
## [181] &quot;6/29/11&quot;         &quot;6/30/11&quot;         &quot;6/5/12&quot;         
## [184] &quot;6/6/12&quot;          &quot;6/7/11&quot;          &quot;6/7/12&quot;         
## [187] &quot;6/8/11&quot;          &quot;6/8/12&quot;          &quot;6/9/11&quot;         
## [190] &quot;7/1/11&quot;          &quot;7/10/12&quot;         &quot;7/11/12&quot;        
## [193] &quot;7/12/11&quot;         &quot;7/12/12&quot;         &quot;7/13/11&quot;        
## [196] &quot;7/13/12&quot;         &quot;7/14/11&quot;         &quot;7/15/11&quot;        
## [199] &quot;7/17/12&quot;         &quot;7/18/12&quot;         &quot;7/20/11&quot;        
## [202] &quot;7/20/12&quot;         &quot;7/21/11&quot;         &quot;7/22/11&quot;        
## [205] &quot;7/24/12&quot;         &quot;7/25/12&quot;         &quot;7/26/11&quot;        
## [208] &quot;7/26/12&quot;         &quot;7/27/11&quot;         &quot;7/27/12&quot;        
## [211] &quot;7/28/11&quot;         &quot;7/29/11&quot;         &quot;7/31/12&quot;        
## [214] &quot;7/6/11&quot;          &quot;7/7/11&quot;          &quot;7/8/11&quot;         
## [217] &quot;8/1/12&quot;          &quot;8/10/11&quot;         &quot;8/10/12&quot;        
## [220] &quot;8/11/11&quot;         &quot;8/12/11&quot;         &quot;8/14/12&quot;        
## [223] &quot;8/15/12&quot;         &quot;8/16/12&quot;         &quot;8/17/11&quot;        
## [226] &quot;8/17/12&quot;         &quot;8/18/11&quot;         &quot;8/19/11&quot;        
## [229] &quot;8/2/12&quot;          &quot;8/21/12&quot;         &quot;8/22/12&quot;        
## [232] &quot;8/23/11&quot;         &quot;8/23/12&quot;         &quot;8/24/11&quot;        
## [235] &quot;8/24/12&quot;         &quot;8/25/11&quot;         &quot;8/26/11&quot;        
## [238] &quot;8/28/12&quot;         &quot;8/29/12&quot;         &quot;8/3/11&quot;         
## [241] &quot;8/3/12&quot;          &quot;8/31/11&quot;         &quot;8/4/11&quot;         
## [244] &quot;8/5/11&quot;          &quot;8/7/12&quot;          &quot;8/8/12&quot;         
## [247] &quot;8/9/11&quot;          &quot;8/9/12&quot;          &quot;9/1/11&quot;         
## [250] &quot;9/11/12&quot;         &quot;9/12/12&quot;         &quot;9/14/11&quot;        
## [253] &quot;9/14/12&quot;         &quot;9/15/11&quot;         &quot;9/16/11&quot;        
## [256] &quot;9/19/12&quot;         &quot;9/2/11&quot;          &quot;9/20/12&quot;        
## [259] &quot;9/21/11&quot;         &quot;9/21/12&quot;         &quot;9/22/11&quot;        
## [262] &quot;9/23/11&quot;         &quot;9/25/12&quot;         &quot;9/26/12&quot;        
## [265] &quot;9/27/12&quot;         &quot;9/28/11&quot;         &quot;9/28/12&quot;        
## [268] &quot;9/29/11&quot;         &quot;9/30/11&quot;         &quot;9/7/11&quot;         
## [271] &quot;9/8/11&quot;          &quot;9/9/11&quot;          &quot;Date&quot;           
## [274] &quot;Date: 5/25/2011&quot; &quot;Date: 5/26/2011&quot; &quot;Date: 5/27/2011&quot;
## [277] &quot;Date: 6/01/2011&quot; &quot;Date: 6/01/2012&quot; &quot;Date: 6/01/2013&quot;
## [280] &quot;Date: 6/01/2014&quot; &quot;Date:09/08/2011&quot;</code></pre>
<p>We see from this list that there are several different formats being used for dates. In most cases the last two digits of the year are displayed. However, in some cases the full 4 digit year is displayed and in others the dates are missing altogether. We will use regular expressions to detect the 4-digit years and replace them with 2-digits. We will also remove any extraneous characters and convert empty strings to missing data by replacing them with NA. Finally we will convert the date from the character data type to the Date data type to facilitate metadata analysis.</p>
<pre class="r"><code>suppl$DATE&lt;-gsub(&quot;(20)([0-9][0-9])&quot;, &quot;\\2&quot;, as.character(suppl$DATE))

suppl&lt;-suppl %&gt;% 
  mutate(DATE=str_replace(DATE, &quot;Date:&quot;, &quot;&quot;)) %&gt;%
  mutate(DATE=str_replace(DATE, &quot;Date&quot;, &quot;&quot;)) %&gt;% 
  mutate_all(funs(na_if(.,&quot;&quot;))) %&gt;%
  mutate(DATE=as.Date(DATE, format = &quot;%m/%d/%y&quot;))

str(suppl)</code></pre>
<pre><code>## &#39;data.frame&#39;:    3207 obs. of  5 variables:
##  $ vdrc_id             : int  105968 105691 105961 105969 105970 105987 107047 103951 107053 107054 ...
##  $ primary_PI          : num  0.42 0.33 0.22 0.23 0.52 0.11 0.62 0.41 0.4 0.43 ...
##  $ primary_SEM         : num  0.044 0.057 0.062 0.075 0.041 0.0533 0.0416 0.0231 0.054 0.074 ...
##  $ DATE                : Date, format: &quot;2011-06-24&quot; &quot;2011-06-24&quot; ...
##  $ physical_abnormality: chr  NA NA NA NA ...</code></pre>
<pre class="r"><code>suppl$DATE %&gt;% 
  na.omit %&gt;% 
  range()</code></pre>
<pre><code>## [1] &quot;2000-01-09&quot; &quot;2014-06-01&quot;</code></pre>
<p>Next we want to format the supplementary data to match the format of the significant_lines table that we previously constructed. To do this we will round the PI and SEMs to two decimal points and code the presence or absence of physical abnormality with “+” and “-” respectively.</p>
<pre class="r"><code>suppl&lt;-suppl %&gt;% 
  mutate(primary_PI = round(primary_PI, 2)) %&gt;% 
  mutate(primary_SEM = abs(round(primary_SEM, 2))) %&gt;%
  mutate(physical_abnormality = replace(physical_abnormality, physical_abnormality == &quot;Present&quot;, &quot;+&quot;)) %&gt;%
  mutate(physical_abnormality = replace(physical_abnormality, is.na(physical_abnormality), &quot;-&quot;)) %&gt;% 
  arrange(vdrc_id)

head(suppl, 10)</code></pre>
<pre><code>##    vdrc_id primary_PI primary_SEM       DATE physical_abnormality
## 1     3047       0.34        0.09 2011-07-29                    -
## 2     3424       0.30        0.15 2013-01-25                    -
## 3     5070       0.40        0.14 2012-06-08                    -
## 4     6212       0.27        0.10 2011-12-14                    -
## 5     7219       0.41        0.04 2012-10-23                    -
## 6     7220       0.12        0.13 2013-03-20                    -
## 7     9524       0.30        0.06 2011-06-28                    -
## 8    10836       0.49        0.07 2013-02-12                    -
## 9    11471       0.16        0.09 2011-07-28                    -
## 10   11784       0.43        0.11 2011-07-29                    -</code></pre>
<p>We will now join the supplementary data with the data from the significant lines using the full_join command from dplyr merging data on the common columns. This command retains all values so if there are discrepancies in the two data sets we should get duplications of some of the VDRC ids.</p>
<pre class="r"><code>all_lines&lt;-full_join(significant_lines, suppl, by=c(&quot;vdrc_id&quot;, &quot;primary_PI&quot;, &quot;primary_SEM&quot;, &quot;physical_abnormality&quot;)) %&gt;% 
  arrange(vdrc_id)</code></pre>
<p>Let’s see what the size of the datasets are. The dimensions of the significant_lines dataframe is:</p>
<pre class="r"><code>dim(significant_lines)</code></pre>
<pre><code>## [1] 599  10</code></pre>
<p>The supplemental data is:</p>
<pre class="r"><code>dim(suppl)</code></pre>
<pre><code>## [1] 3207    5</code></pre>
<p>and the merged data is:</p>
<pre class="r"><code>dim(all_lines)</code></pre>
<pre><code>## [1] 3241   11</code></pre>
<p>We expect that the supplemental data would contain the significant_lines dataset but we notice that there is a discrepancy of 34 suggesting that there is a problem in 34 of the lines. We can take a closer look at these lines by counting how many are duplicated:</p>
<pre class="r"><code>which(duplicated(all_lines$vdrc_id)==TRUE) %&gt;% length()</code></pre>
<pre><code>## [1] 34</code></pre>
<p>Since 34 of the lines are duplicated this completely explains the discrepancy. Now we want to find the reason behind this discrepancy. To do this we need to look at the rows that contain the duplicated vdrc_id. One way to do that is to use the VDRC ids to retrieve the rows. I couldn’t find a very simple way of doing this so I split it into two parts. The first part assigns all the VDRC ids to a vector we call dup. We then use the dplyr filter command to pull out the rows with the VDRC ids in dup. Let’s look at the first 15 rows.</p>
<pre class="r"><code>dup&lt;-all_lines[which(duplicated(all_lines$vdrc_id)==TRUE),]$vdrc_id
filter(all_lines, vdrc_id %in% dup) %&gt;% head(15)</code></pre>
<pre><code>##    vdrc_id cg_number primary_PI primary_SEM secondary_PI secondary_SEM
## 1    48984    CG8451       0.10        0.17         0.22          0.10
## 2    48984      &lt;NA&gt;       0.10        0.16           NA            NA
## 3   100432    CG4922       0.18        0.03         0.11          0.05
## 4   100432      &lt;NA&gt;       0.18        0.04           NA            NA
## 5   100652   CG32017       0.18        0.05         0.13          0.04
## 6   100652      &lt;NA&gt;       0.18        0.04           NA            NA
## 7   100691    CG9949       0.25        0.03         0.25          0.02
## 8   100691      &lt;NA&gt;       0.25        0.02           NA            NA
## 9   100706    CG1470       0.62        0.07         0.61          0.02
## 10  100706      &lt;NA&gt;       0.62        0.06           NA            NA
## 11  100947   CG13933      -0.19        0.03         0.14          0.02
## 12  100947      &lt;NA&gt;       0.16        0.03           NA            NA
## 13  101047   CG32016       0.14        0.09         0.12          0.08
## 14  101047      &lt;NA&gt;       0.14        0.08           NA            NA
## 15  101372   CG42540       0.24        0.07         0.21          0.02
##    physical_abnormality mean_activity_difference act_sig change_in_memory
## 1                     -                       NA    &lt;NA&gt;                -
## 2                     -                       NA    &lt;NA&gt;             &lt;NA&gt;
## 3                     -                     6.03                        -
## 4                     -                       NA    &lt;NA&gt;             &lt;NA&gt;
## 5                     -                    -2.76                        -
## 6                     -                       NA    &lt;NA&gt;             &lt;NA&gt;
## 7                     -                    14.65                        -
## 8                     -                       NA    &lt;NA&gt;             &lt;NA&gt;
## 9                     -                    24.07                        +
## 10                    -                       NA    &lt;NA&gt;             &lt;NA&gt;
## 11                    -                    34.29                        -
## 12                    -                       NA    &lt;NA&gt;             &lt;NA&gt;
## 13                    -                    20.15                        -
## 14                    -                       NA    &lt;NA&gt;             &lt;NA&gt;
## 15                    +                       NA    &lt;NA&gt;                -
##          DATE
## 1        &lt;NA&gt;
## 2  2013-01-22
## 3        &lt;NA&gt;
## 4  2011-06-07
## 5        &lt;NA&gt;
## 6  2011-07-01
## 7        &lt;NA&gt;
## 8  2011-06-09
## 9        &lt;NA&gt;
## 10 2011-07-01
## 11       &lt;NA&gt;
## 12 2011-06-14
## 13       &lt;NA&gt;
## 14 2011-06-15
## 15       &lt;NA&gt;</code></pre>
<p>There are two things we can see from this snippet. One is that the SEMs are different in certain cases and the second is that the PIs are different in other cases (e.g. vdrc_id=100947. The small differences in the SEMs are easy to explain since they reflect idiosyncratic behavior of the round() function we used earlier to round the values to 2 decimal places. The differences in PI are larger but fewer in number so could reflect a data entry error. In all of these cases we will use the values from the manuscript since these are more likely to be correct.</p>
<p>First we will use anti-join() to remove all of the significant lines from the supplementary data and then use bind_rows() to add back these lines and then arrange them by ascending order of the VDRC id.</p>
<pre class="r"><code>all_lines &lt;- anti_join(suppl, significant_lines, by=c(&quot;vdrc_id&quot;)) %&gt;%
  bind_rows(significant_lines) %&gt;% 
  arrange(vdrc_id)</code></pre>
<p>However, with this approach we lose the dates for the significant lines. To add these back in we will select the vdrc_id and DATE columns from suppl and join them to the all_lines dataframe.</p>
<pre class="r"><code>all_lines &lt;- suppl %&gt;% 
  dplyr::select(vdrc_id, DATE) %&gt;% left_join(all_lines, by=&quot;vdrc_id&quot;) %&gt;% 
  dplyr::select(vdrc_id, cg_number, date_primary_expt=DATE.x, primary_PI, primary_SEM, secondary_PI, secondary_SEM, mean_activity_difference, act_sig, physical_abnormality, change_in_memory)</code></pre>
<p>At this point we have all of the data in one table but the only consistent identifiers are the vdrc_id. We would like to add more useful identifiers such as gene symbols, Entrez Gene ID and Flybase IDs. The stock list from VDRC has some of these identifiers already and can be downloaded from the following link: <a href="https://stockcenter.vdrc.at/control/fullCatalogueExcel" class="uri">https://stockcenter.vdrc.at/control/fullCatalogueExcel</a>. We will use the readWorksheetFromFile command of the XLConnect package to load this file into a new dataframe and look at the variables in the dataframe using str()</p>
<pre class="r"><code>library(XLConnect)
VDRC &lt;- readWorksheetFromFile(&quot;~/Documents/R_projects/Walkinshaw/REPORT_VdrcCatalogue.xls&quot;, sheet = 1)

str(VDRC)</code></pre>
<pre><code>## &#39;data.frame&#39;:    33103 obs. of  15 variables:
##  $ vdrc.id      : num  836 837 839 841 842 843 845 846 848 849 ...
##  $ construct.id : num  37 37 38 44 45 45 47 47 49 49 ...
##  $ library      : chr  &quot;GD&quot; &quot;GD&quot; &quot;GD&quot; &quot;GD&quot; ...
##  $ cg.number    : chr  &quot;CG5819&quot; &quot;CG5819&quot; &quot;CG7121&quot; &quot;CG4007&quot; ...
##  $ on.targets.  : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ off.targets  : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ s19          : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ can.repeats  : num  2 2 2 3 2 2 2 2 2 2 ...
##  $ viability    : chr  &quot;viable&quot; &quot;viable&quot; &quot;viable&quot; &quot;sterile&quot; ...
##  $ chromosome.nr: num  3 1 2 3 2 3 3 2 2 2 ...
##  $ status       : chr  &quot;available&quot; &quot;unavailable&quot; &quot;available&quot; &quot;unavailable&quot; ...
##  $ vector       : chr  NA NA NA NA ...
##  $ landing_site : chr  NA NA NA NA ...
##  $ fb.numbers   : chr  &quot;\nFBgn0034717&quot; &quot;FBgn0034717&quot; &quot;FBgn0026760&quot; &quot;FBgn0020391\nFBgn0022790\nFBgn0021861\nFBgn0082206&quot; ...
##  $ synonyms     : chr  &quot;\ntartan/capricious-like\nCT18212\n5819&quot; &quot;\ntartan/capricious-like\n5819\nCT18212&quot; &quot;tehao\ntoll\ntoll receptor\nToll 5\nTehao\nToll-5\nTl-5\nTho\ndToll5\nCT22017\ndTLR5&quot; &quot;anon-WO2004063362.79\nNrk\nNeurospecific receptor kinase\nnrtk_dros\nDmHD-434\nDrosophila neurospecific recepto&quot;| __truncated__ ...</code></pre>
<p>There are a number of identifiers available but many of these are non-unique such as the FBgn numbers and names. The CG numbers are the most stable and that is what we will use for the time being but will use a webservice to add other identifiers later.</p>
<pre class="r"><code>all_lines&lt;-VDRC %&gt;% 
  dplyr::select(vdrc.id, cg.number, library, on.targets., off.targets, s19, can.repeats, chromosome.nr) %&gt;% 
  inner_join(all_lines, by=c(&quot;vdrc.id&quot;=&quot;vdrc_id&quot;)) %&gt;%
  dplyr::select(-&quot;cg_number&quot;) %&gt;% 
  dplyr::rename(vdrc_id = vdrc.id, cg_number = cg.number, on_targets=on.targets., off_targets=off.targets, can_repeats=can.repeats, chromosome=chromosome.nr)

head(all_lines, 10)</code></pre>
<pre><code>##    vdrc_id cg_number library on_targets off_targets  s19 can_repeats
## 1     3047   CG17348      GD          1           0 1.00           2
## 2     3424    CG8451      GD          1           0 1.00           2
## 3     5070    CG8339      GD          1           0 1.00           2
## 4     6212   CG33956      GD          1           1 1.00           3
## 5     7219    CG3171      GD          1           1 1.00           2
## 6     7220    CG3171      GD          1           1 1.00           2
## 7    10836    CG4976      GD          1           2 0.99           5
## 8    11471   CG33517      GD          1           1 1.00           3
## 9    11784    CG5099      GD          1           0 1.00           3
## 10   11816   CG42260      GD          1           0 1.00           2
##    chromosome date_primary_expt primary_PI primary_SEM secondary_PI
## 1           2        2011-07-29       0.34        0.09           NA
## 2           3        2013-01-25       0.30        0.15           NA
## 3           3        2012-06-08       0.40        0.14           NA
## 4           3        2011-12-14       0.27        0.10           NA
## 5           3        2012-10-23       0.41        0.04           NA
## 6           3        2013-03-20       0.12        0.13           NA
## 7           2        2013-02-12       0.49        0.07           NA
## 8           2        2011-07-28       0.16        0.09         0.13
## 9           1        2011-07-29       0.43        0.11           NA
## 10          3        2011-08-18       0.64        0.08           NA
##    secondary_SEM mean_activity_difference act_sig physical_abnormality
## 1             NA                       NA    &lt;NA&gt;                    -
## 2             NA                       NA    &lt;NA&gt;                    -
## 3             NA                       NA    &lt;NA&gt;                    -
## 4             NA                       NA    &lt;NA&gt;                    -
## 5             NA                       NA    &lt;NA&gt;                    -
## 6             NA                       NA    &lt;NA&gt;                    -
## 7             NA                       NA    &lt;NA&gt;                    -
## 8            0.1                    -5.63                            -
## 9             NA                       NA    &lt;NA&gt;                    -
## 10            NA                       NA    &lt;NA&gt;                    -
##    change_in_memory
## 1              &lt;NA&gt;
## 2              &lt;NA&gt;
## 3              &lt;NA&gt;
## 4              &lt;NA&gt;
## 5              &lt;NA&gt;
## 6              &lt;NA&gt;
## 7              &lt;NA&gt;
## 8                 -
## 9              &lt;NA&gt;
## 10             &lt;NA&gt;</code></pre>
<p>Next we will use the biomart service of EBI to get current Entrez Gene IDs, FlyBase numbers and gene symbols using the list of CG numbers from the all_lines dataframe. The biomaRt package allows very convenient programmatic access to biomart.</p>
<pre class="r"><code>library(biomaRt)
ensembl = useEnsembl(biomart=&quot;ensembl&quot;, dataset=&quot;dmelanogaster_gene_ensembl&quot;, version=90)
genenames &lt;- getBM(attributes=c(&#39;external_gene_name&#39;,&#39;flybase_gene_id&#39;, &#39;flybasecgid_gene&#39;), filters = &#39;flybasecgid_gene&#39;, values = unique(all_lines$cg_number), mart = ensembl)</code></pre>
<p>Let’s check the size of the genenames dataframe to get an idea if we were able to map every identifier.</p>
<pre class="r"><code>dim(genenames)</code></pre>
<pre><code>## [1] 2997    3</code></pre>
<p>The genenames dataframe has 3154 rows instead of the 3206 that we expected, a discrepancy of 52. Let’s find if there are any CG numbers that were not mapped.</p>
<pre class="r"><code>all_lines$cg_number[(all_lines$cg_number %in% genenames$flybasecgid_gene)==FALSE]</code></pre>
<pre><code>##  [1] &quot;CGnone&quot;  &quot;CGnone&quot;  &quot;CGnone&quot;  &quot;CGnone&quot;  &quot;CGnone&quot;  &quot;CGnone&quot;  &quot;CGnone&quot; 
##  [8] &quot;CGnone&quot;  &quot;CGnone&quot;  &quot;CGnone&quot;  &quot;CGnone&quot;  &quot;CGnone&quot;  &quot;CGnone&quot;  &quot;CGnone&quot; 
## [15] &quot;CG40378&quot; &quot;CGnone&quot;  &quot;CGnone&quot;  &quot;CGnone&quot;  &quot;CGnone&quot;  &quot;CGnone&quot;  &quot;CGnone&quot; 
## [22] &quot;CGnone&quot;  &quot;CGnone&quot;  &quot;CGnone&quot;  &quot;CGnone&quot;</code></pre>
<p>The striking thing is that there are 24 instances of CGnone in all_lines which indicates that the RNAi lines against these. There is also one real CG number, CG40378, and checking on Flybase we notice that it has been replaced by CG45781. We will remove the CGnone lines, change CG40378 to CG45781 and reuse Biomart to fill in the rest of the information.</p>
<pre class="r"><code>all_lines &lt;- all_lines %&gt;% 
  filter(cg_number != &quot;CGnone&quot;)

all_lines$cg_number &lt;- all_lines$cg_number %&gt;% 
  str_replace(&quot;CG40378&quot;, &quot;CG45781&quot;)

genenames &lt;- getBM(attributes=c(&#39;external_gene_name&#39;,&#39;flybase_gene_id&#39;, &#39;flybasecgid_gene&#39;), filters = &#39;flybasecgid_gene&#39;, values = unique(all_lines$cg_number), mart = ensembl)</code></pre>
<p>Finally, we can add the gene symbols and flybase gene ids to the all_lines dataframe using an inner join with the genenames dataframe.</p>
<pre class="r"><code>all_lines &lt;- inner_join(genenames, all_lines, by=c(&quot;flybasecgid_gene&quot;=&quot;cg_number&quot;)) %&gt;% 
  dplyr::select(4,1,2,3,5:19) %&gt;% 
  rename(cg_number=flybasecgid_gene)

head(all_lines, 10)</code></pre>
<pre><code>##    vdrc_id external_gene_name flybase_gene_id cg_number library on_targets
## 1   101031             Zip71B     FBgn0036461   CG10006      KK          1
## 2    28150            Galphai     FBgn0001104   CG10060      GD          1
## 3   100587                emc     FBgn0000575    CG1007      KK          1
## 4   100007            CG10137     FBgn0032800   CG10137      KK          1
## 5   100226                x16     FBgn0028554   CG10203      KK          1
## 6   100204                Zif     FBgn0037446   CG10267      KK          1
## 7   100180               D19A     FBgn0022935   CG10269      KK          1
## 8   101168            CG10283     FBgn0032681   CG10283      KK          1
## 9   100775                how     FBgn0264491   CG10293      KK          1
## 10  101567             nonA-l     FBgn0015520   CG10328      KK          1
##    off_targets  s19 can_repeats chromosome date_primary_expt primary_PI
## 1            2 0.99           3          2        2011-07-08       0.47
## 2            0 1.00           2          3        2012-10-02       0.49
## 3            0 1.00           2          2        2011-06-09       0.39
## 4            1 0.99           2          2        2011-08-23       0.44
## 5            0 1.00           3          2        2013-03-21       0.12
## 6            0 1.00           3          2        2011-11-02       0.60
## 7            0 1.00           3          2        2011-11-02       0.46
## 8            0 1.00           3          2        2012-10-05       0.49
## 9            1 1.00           2          2        2011-08-05       0.40
## 10           1 1.00           3          2        2011-09-15       0.34
##    primary_SEM secondary_PI secondary_SEM mean_activity_difference act_sig
## 1         0.03           NA            NA                       NA    &lt;NA&gt;
## 2         0.08           NA            NA                       NA    &lt;NA&gt;
## 3         0.05           NA            NA                       NA    &lt;NA&gt;
## 4         0.07           NA            NA                       NA    &lt;NA&gt;
## 5         0.12         0.18          0.14                       NA    &lt;NA&gt;
## 6         0.07           NA            NA                       NA    &lt;NA&gt;
## 7         0.03           NA            NA                       NA    &lt;NA&gt;
## 8         0.03           NA            NA                       NA    &lt;NA&gt;
## 9         0.05           NA            NA                       NA    &lt;NA&gt;
## 10        0.04           NA            NA                       NA    &lt;NA&gt;
##    physical_abnormality change_in_memory
## 1                     -             &lt;NA&gt;
## 2                     -             &lt;NA&gt;
## 3                     -             &lt;NA&gt;
## 4                     +             &lt;NA&gt;
## 5                     -                -
## 6                     -             &lt;NA&gt;
## 7                     +             &lt;NA&gt;
## 8                     -             &lt;NA&gt;
## 9                     -             &lt;NA&gt;
## 10                    -             &lt;NA&gt;</code></pre>
<div id="conclusions" class="section level3">
<h3>Conclusions</h3>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 3.5.1 (2018-07-02)
## Platform: x86_64-apple-darwin15.6.0 (64-bit)
## Running under: macOS  10.14.2
## 
## Matrix products: default
## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib
## 
## locale:
## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] XLConnect_0.2-15     XLConnectJars_0.2-15 janitor_1.1.1       
##  [4] bindrcpp_0.2.2       forcats_0.3.0        stringr_1.3.1       
##  [7] dplyr_0.7.6          purrr_0.2.5          readr_1.1.1         
## [10] tidyr_0.8.1          tibble_1.4.2         ggplot2_3.0.0       
## [13] tidyverse_1.2.1      tabulizer_0.2.2     
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_0.2.4    xfun_0.3            rJava_0.9-10       
##  [4] haven_1.1.2         lattice_0.20-35     colorspace_1.3-2   
##  [7] htmltools_0.3.6     yaml_2.2.0          rlang_0.2.2        
## [10] pillar_1.3.0        withr_2.1.2         glue_1.3.0         
## [13] modelr_0.1.2        readxl_1.1.0        bindr_0.1.1        
## [16] plyr_1.8.4          munsell_0.5.0       blogdown_0.8       
## [19] gtable_0.2.0        cellranger_1.1.0    rvest_0.3.2        
## [22] evaluate_0.11       knitr_1.20          broom_0.5.0        
## [25] Rcpp_0.12.18        tabulizerjars_1.0.1 backports_1.1.2    
## [28] scales_1.0.0        jsonlite_1.5        hms_0.4.2          
## [31] png_0.1-7           digest_0.6.16       stringi_1.2.4      
## [34] bookdown_0.7        grid_3.5.1          rprojroot_1.3-2    
## [37] cli_1.0.0           tools_3.5.1         magrittr_1.5       
## [40] lazyeval_0.2.1      crayon_1.3.4        pkgconfig_2.0.2    
## [43] xml2_1.2.0          lubridate_1.7.4     assertthat_0.2.0   
## [46] rmarkdown_1.10      httr_1.3.1          rstudioapi_0.7     
## [49] R6_2.2.2            nlme_3.1-137        compiler_3.5.1</code></pre>
</div>
<div id="references" class="section level3">
<h3>References</h3>
<p>Stat454
Variance Explained
Stack Overflow</p>
<hr />
</div>
